{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DECiSION incl. data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seeds and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoff/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "from numpy.random import seed\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "import random\n",
    "random.seed = RANDOM_STATE\n",
    "\n",
    "import DECiSION_settings as settings\n",
    "\n",
    "from dltoolkit.utils.generic import model_architecture_to_file, model_summary_to_file, list_images\n",
    "from dltoolkit.utils.image import normalise, normalise_single, standardise, standardise_single,\\\n",
    "    mean_subtraction, clahe_equalization, clahe_equalization_single, adjust_gamma_single\n",
    "from dltoolkit.nn.segment import UNet_NN\n",
    "from dltoolkit.utils.visual import plot_training_history\n",
    "from dltoolkit.iomisc import HDF5Writer, HDF5Reader, HDF5Generator_Segment\n",
    "\n",
    "from thesis_common import convert_img_to_pred, convert_pred_to_img,\\\n",
    "    group_images, show_image, read_preprocess_image, read_preprocess_groundtruth\n",
    "from thesis_metric_loss import dice_coef, weighted_pixelwise_crossentropy_loss\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os, cv2, time, progressbar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hdf5_db(imgs_list, dn_name, img_path, img_shape, key, ext, settings, is_mask=False):\n",
    "    \"\"\"\n",
    "    Create a HDF5 file using a list of paths to individual images to be written to the data set\n",
    "    :param imgs_list: list of image paths\n",
    "    :param dn_name: becomes part of the HDF5 file name\n",
    "    :param img_path: path to the location of the `images` and `groundtruths` subfolders\n",
    "    :param img_shape: shape of the images being written to the data set\n",
    "    :param key: key to use for the data set\n",
    "    :param ext: extension of the HDF5 file name\n",
    "    :param settings: holds settings\n",
    "    :param is_mask: True if masks are being written, False if not\n",
    "    :return: the full path to the HDF5 file\n",
    "    \"\"\"\n",
    "    # Construct the name of the database\n",
    "    tmp_name = dn_name + (\"_masks\" if is_mask else \"_imgs\")\n",
    "    output_path = os.path.join(os.path.dirname(img_path), tmp_name) + ext\n",
    "    print(output_path)\n",
    "\n",
    "    # Prepare the HDF5 writer, which expects a label vector. Because this is a segmentation problem just pass None\n",
    "    # hdf5_writer = HDF5Writer((len(imgs_list), img_shape[0], img_shape[1], img_shape[2]), output_path,\n",
    "    hdf5_writer = HDF5Writer(((len(imgs_list),) + img_shape),\n",
    "                             output_path=output_path,\n",
    "                             feat_key=key,\n",
    "                             label_key=None,\n",
    "                             del_existing=True,\n",
    "                             buf_size=len(imgs_list),\n",
    "                             dtype_feat=np.float32 if not is_mask else np.uint8\n",
    "                             )\n",
    "    # Prepare for CLAHE histogram equalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(16, 16))\n",
    "\n",
    "    # Loop through all images\n",
    "    widgets = [\"Creating HDF5 database \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(imgs_list), widgets=widgets).start()\n",
    "    for i, img in enumerate(imgs_list):\n",
    "        image = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Crop to the region of interest\n",
    "        image = image[settings.IMG_CROP_HEIGHT:image.shape[0]-settings.IMG_CROP_HEIGHT,\n",
    "                      settings.IMG_CROP_WIDTH:image.shape[1]-settings.IMG_CROP_WIDTH]\n",
    "\n",
    "        # Apply pre-processing\n",
    "        if is_mask:\n",
    "            # Apply binary thresholding to ground truth masks\n",
    "            _, image = cv2.threshold(image, settings.MASK_BINARY_THRESHOLD, settings.MASK_BLOODVESSEL, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Convert to the format produced by the model\n",
    "            # image = convert_img_to_pred(np.array([image]), settings, settings.VERBOSE)\n",
    "        else:\n",
    "            # Apply preprocessing to images (not to ground truth masks)\n",
    "            # Apply CLAHE histogram equalization\n",
    "            image = clahe.apply(image)\n",
    "\n",
    "            # Normalise between -0.5 and 0.5\n",
    "            image = (image/255.0-0.5).astype(np.float32)\n",
    "\n",
    "        # Reshape from (height, width) to (height, width, 1)\n",
    "        image = image.reshape((img_shape[0], img_shape[1], img_shape[2]))\n",
    "\n",
    "        hdf5_writer.add([image], None)\n",
    "        pbar.update(i)\n",
    "\n",
    "    pbar.finish()\n",
    "    hdf5_writer.close()\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def perform_hdf5_conversion(settings):\n",
    "    # Prepare the path to the training images and ground truths\n",
    "    img_exts = \".jpg\"\n",
    "    img_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_IMAGES)\n",
    "    msk_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_GROUND_TRUTH)\n",
    "    test_path = os.path.join(settings.TEST_PATH, settings.FLDR_IMAGES)\n",
    "\n",
    "    # Create a list of paths to the individual patient folders\n",
    "    patient_fld_imgs = sorted([os.path.join(img_path, e.name) for e in os.scandir(img_path) if e.is_dir()])\n",
    "    patient_fld_masks = sorted([os.path.join(msk_path, e.name) for e in os.scandir(msk_path) if e.is_dir()])\n",
    "    test_imgs = sorted(list(list_images(basePath=test_path, validExts=img_exts)))\n",
    "\n",
    "    # Obtain a list of paths to the training images and ground truths for each patient\n",
    "    img_list = []\n",
    "    msk_list = []\n",
    "    for patient_ix, (p_fld_imgs, p_fld_masks) in enumerate(zip(patient_fld_imgs, patient_fld_masks)):\n",
    "        img_list.extend(sorted(list(list_images(basePath=p_fld_imgs,\n",
    "                                                validExts=img_exts)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "        msk_list.extend(sorted(list(list_images(basePath=p_fld_masks,\n",
    "                                                validExts=img_exts)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "\n",
    "    assert(len(img_list) == len(msk_list))\n",
    "\n",
    "    # Split the training set into a training and validation set\n",
    "    train_img, val_img, train_msk, val_msk = train_test_split(img_list, msk_list,\n",
    "                                                              test_size=settings.TRN_TRAIN_VAL_SPLIT,\n",
    "                                                              random_state=settings.RANDOM_STATE,\n",
    "                                                              shuffle=True)\n",
    "    \n",
    "    # Create the HDF5 data sets\n",
    "    output_paths = []\n",
    "\n",
    "    # Training images\n",
    "    output_paths.append(create_hdf5_db(train_img, \"train\", img_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings))\n",
    "\n",
    "    # Training ground truths\n",
    "    output_paths.append(create_hdf5_db(train_msk, \"train\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    # Validation images\n",
    "    output_paths.append(create_hdf5_db(val_img, \"val\", img_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings))\n",
    "\n",
    "    # Validation ground truths\n",
    "    output_paths.append(create_hdf5_db(val_msk, \"val\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    # Test images (no ground truths available, no need to split). The assumption is only\n",
    "    # relevant images are placed in the test folder, i.e. the pipeline will not exclude\n",
    "    # any slices\n",
    "    output_paths.append(create_hdf5_db(test_imgs, \"test\", test_path,\n",
    "                                        (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                        key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings))\n",
    "\n",
    "    return output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database  30% |##########                         | ETA:  0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converting images to HDF5\n",
      "Check train data: ../data/MSC8002/training/images/patient_1/S572080069.jpg = ../data/MSC8002/training/groundtruths/patient_1/S57208Filter0069.jpg\n",
      "  Check val data: ../data/MSC8002/training/images/patient_1/S572080067.jpg = ../data/MSC8002/training/groundtruths/patient_1/S57208Filter0067.jpg\n",
      "Num train: 9, num val: 1\n",
      "../data/MSC8002/training/train_imgs.h5\n",
      "../data/MSC8002/training/train_masks.h5\n",
      "../data/MSC8002/training/val_imgs.h5\n",
      "../data/MSC8002/training/val_masks.h5\n",
      "../data/MSC8002/test/test_imgs.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# Convert image files to HDF5\n",
    "if settings.IS_DEVELOPMENT:\n",
    "    print(\"\\n--- Converting images to HDF5\")\n",
    "    hdf5_paths = perform_hdf5_conversion(settings)\n",
    "else:\n",
    "    # During development avoid performing HDF5 conversion for every run\n",
    "    hdf5_paths = [\"../data/MSC8002/training/train_imgs.h5\",\n",
    "                  \"../data/MSC8002/training/train_masks.h5\",\n",
    "                  \"../data/MSC8002/training/val_imgs.h5\",\n",
    "                  \"../data/MSC8002/training/val_masks.h5\"\n",
    "                  \"../data/MSC8002/test/test_imgs.h5\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [1.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "# Print class distribution\n",
    "class_weights = [settings.CLASS_WEIGHT_BACKGROUND, settings.CLASS_WEIGHT_BLOODVESSEL]\n",
    "print(\"Class distribution: {}\".format(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the U-Net model\n",
    "unet = UNet_NN(img_height=settings.IMG_HEIGHT,\n",
    "               img_width=settings.IMG_WIDTH,\n",
    "               img_channels=settings.IMG_CHANNELS,\n",
    "               num_classes=settings.NUM_CLASSES)\n",
    "\n",
    "# model = unet.build_model_sigmoid()\n",
    "# model = unet.build_model_flatten()\n",
    "model = unet.build_model_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some path strings\n",
    "model_path = os.path.join(settings.MODEL_PATH, \"DECiSION_\" + unet.title + \"_ep{}.model\".format(settings.TRN_NUM_EPOCH))\n",
    "summ_path = os.path.join(settings.OUTPUT_PATH, \"DECiSION_\" + unet.title + \"_model_summary.txt\")\n",
    "csv_path = os.path.join(settings.OUTPUT_PATH, \"DECiSION_\" + unet.title + \"_training_ep{}_bs{}.csv\".format(settings.TRN_NUM_EPOCH,\n",
    "                                                                                            settings.TRN_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/print model architecture information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 240, 240, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 240, 240, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 240, 240, 32) 9248        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 120, 120, 32) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 120, 120, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 120, 120, 64) 36928       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 60, 60, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 60, 60, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 60, 60, 128)  147584      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 120, 120, 64) 32832       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 120, 120, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 120, 120, 64) 73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 120, 120, 64) 36928       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 240, 240, 32) 8224        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 240, 240, 64) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 240, 240, 32) 18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 240, 240, 32) 9248        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 240, 240, 2)  66          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 240, 240, 2)  0           conv2d_11[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 465,986\n",
      "Trainable params: 465,986\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the architecture to the console, a text file and an image\n",
    "model.summary()\n",
    "model_summary_to_file(model, summ_path)\n",
    "model_architecture_to_file(unet.model, settings.OUTPUT_PATH + \"DECiSION_\" + unet.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimiser, loss function and metrics\n",
    "opt = Adam()\n",
    "metrics = [dice_coef]\n",
    "loss = weighted_pixelwise_crossentropy_loss(class_weights)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num trn samples: 9\n",
      " num val samples: 1\n",
      " steps_per_epoch: 9.0\n",
      "validation_steps: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training set generator using data augmentation\n",
    "rdr_train = HDF5Generator_Segment(hdf5_paths[0], hdf5_paths[1],\n",
    "                                  batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                  num_classes=settings.NUM_CLASSES,\n",
    "                                  converter=convert_img_to_pred, \n",
    "#                                   data_gen_args=data_gen_args,\n",
    "                                  feat_key=settings.HDF5_KEY)\n",
    "gen_train = rdr_train.generator(num_epochs=settings.TRN_NUM_EPOCH)\n",
    "\n",
    "# Validation set generator (does NOT use data augmentation)\n",
    "rdr_val = HDF5Generator_Segment(hdf5_paths[2], hdf5_paths[3],\n",
    "                                batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                num_classes=settings.NUM_CLASSES,\n",
    "                                converter=convert_img_to_pred, \n",
    "                                feat_key=settings.HDF5_KEY)\n",
    "gen_val = rdr_val.generator(num_epochs=settings.TRN_NUM_EPOCH)\n",
    "\n",
    "print(\" num trn samples: {}\".format(rdr_train.num_images()))\n",
    "print(\" num val samples: {}\".format(rdr_val.num_images()))\n",
    "\n",
    "print(\" steps_per_epoch: {}\".format(rdr_train.num_images()/settings.TRN_BATCH_SIZE))\n",
    "print(\"validation_steps: {}\".format(rdr_val.num_images()/settings.TRN_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train WITH a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare callbacks\n",
    "callbacks = [ModelCheckpoint(model_path, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "             EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0,\n",
    "                           patience=settings.TRN_EARLY_PATIENCE,\n",
    "                           verbose=0,\n",
    "                           mode=\"auto\"),\n",
    "             CSVLogger(csv_path, append=False),\n",
    "             ]\n",
    "\n",
    "# Fit the model using generators and a validation set\n",
    "start_time = time.time()\n",
    "hist = model.fit_generator(gen_train,\n",
    "                 epochs=settings.TRN_NUM_EPOCH,\n",
    "                 steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                 verbose=2,\n",
    "                 validation_data=gen_val,\n",
    "                 validation_steps=rdr_val.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                 shuffle=True,\n",
    "                 callbacks=callbacks)\n",
    "\n",
    "print(\"\\n\\nElapsed training time: {} min\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train WITHOUT a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 7s - loss: 33376.1421 - dice_coef: 0.6436\n",
      "\n",
      "Epoch 00001: loss improved from inf to 33376.14214, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 2/100\n",
      " - 6s - loss: 16312.0703 - dice_coef: 0.9033\n",
      "\n",
      "Epoch 00002: loss improved from 33376.14214 to 16312.07031, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 3/100\n",
      " - 6s - loss: 14171.6432 - dice_coef: 0.9211\n",
      "\n",
      "Epoch 00003: loss improved from 16312.07031 to 14171.64323, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 4/100\n",
      " - 6s - loss: 12697.4178 - dice_coef: 0.9364\n",
      "\n",
      "Epoch 00004: loss improved from 14171.64323 to 12697.41775, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 5/100\n",
      " - 7s - loss: 11241.8515 - dice_coef: 0.9412\n",
      "\n",
      "Epoch 00005: loss improved from 12697.41775 to 11241.85145, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 6/100\n",
      " - 6s - loss: 9668.0025 - dice_coef: 0.9487\n",
      "\n",
      "Epoch 00006: loss improved from 11241.85145 to 9668.00250, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 7/100\n",
      " - 6s - loss: 7934.5594 - dice_coef: 0.9587\n",
      "\n",
      "Epoch 00007: loss improved from 9668.00250 to 7934.55935, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 8/100\n",
      " - 6s - loss: 6397.1090 - dice_coef: 0.9665\n",
      "\n",
      "Epoch 00008: loss improved from 7934.55935 to 6397.10900, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 9/100\n",
      " - 6s - loss: 5173.2628 - dice_coef: 0.9770\n",
      "\n",
      "Epoch 00009: loss improved from 6397.10900 to 5173.26280, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 10/100\n",
      " - 6s - loss: 4458.1993 - dice_coef: 0.9807\n",
      "\n",
      "Epoch 00010: loss improved from 5173.26280 to 4458.19933, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 11/100\n",
      " - 6s - loss: 3897.7697 - dice_coef: 0.9838\n",
      "\n",
      "Epoch 00011: loss improved from 4458.19933 to 3897.76975, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 12/100\n",
      " - 6s - loss: 3596.0655 - dice_coef: 0.9855\n",
      "\n",
      "Epoch 00012: loss improved from 3897.76975 to 3596.06548, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 13/100\n",
      " - 7s - loss: 3493.4718 - dice_coef: 0.9864\n",
      "\n",
      "Epoch 00013: loss improved from 3596.06548 to 3493.47179, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 14/100\n",
      " - 7s - loss: 3436.6374 - dice_coef: 0.9872\n",
      "\n",
      "Epoch 00014: loss improved from 3493.47179 to 3436.63740, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 15/100\n",
      " - 7s - loss: 3283.1183 - dice_coef: 0.9877\n",
      "\n",
      "Epoch 00015: loss improved from 3436.63740 to 3283.11833, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 16/100\n",
      " - 7s - loss: 3232.8521 - dice_coef: 0.9891\n",
      "\n",
      "Epoch 00016: loss improved from 3283.11833 to 3232.85208, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 17/100\n",
      " - 7s - loss: 3010.4021 - dice_coef: 0.9899\n",
      "\n",
      "Epoch 00017: loss improved from 3232.85208 to 3010.40210, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 18/100\n",
      " - 8s - loss: 2903.7359 - dice_coef: 0.9913\n",
      "\n",
      "Epoch 00018: loss improved from 3010.40210 to 2903.73595, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 19/100\n",
      " - 8s - loss: 2841.0788 - dice_coef: 0.9916\n",
      "\n",
      "Epoch 00019: loss improved from 2903.73595 to 2841.07878, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 20/100\n",
      " - 9s - loss: 2860.0464 - dice_coef: 0.9914\n",
      "\n",
      "Epoch 00020: loss did not improve\n",
      "Epoch 21/100\n",
      " - 9s - loss: 2929.4760 - dice_coef: 0.9909\n",
      "\n",
      "Epoch 00021: loss did not improve\n",
      "Epoch 22/100\n",
      " - 9s - loss: 2972.7769 - dice_coef: 0.9900\n",
      "\n",
      "Epoch 00022: loss did not improve\n",
      "Epoch 23/100\n",
      " - 9s - loss: 2870.0786 - dice_coef: 0.9916\n",
      "\n",
      "Epoch 00023: loss did not improve\n",
      "Epoch 24/100\n",
      " - 9s - loss: 2819.4976 - dice_coef: 0.9923\n",
      "\n",
      "Epoch 00024: loss improved from 2841.07878 to 2819.49761, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 25/100\n",
      " - 9s - loss: 2877.0654 - dice_coef: 0.9907\n",
      "\n",
      "Epoch 00025: loss did not improve\n",
      "Epoch 26/100\n",
      " - 9s - loss: 2928.0298 - dice_coef: 0.9911\n",
      "\n",
      "Epoch 00026: loss did not improve\n",
      "Epoch 27/100\n",
      " - 9s - loss: 2786.5448 - dice_coef: 0.9922\n",
      "\n",
      "Epoch 00027: loss improved from 2819.49761 to 2786.54481, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 28/100\n",
      " - 9s - loss: 2734.3660 - dice_coef: 0.9924\n",
      "\n",
      "Epoch 00028: loss improved from 2786.54481 to 2734.36597, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 29/100\n",
      " - 9s - loss: 2754.4356 - dice_coef: 0.9927\n",
      "\n",
      "Epoch 00029: loss did not improve\n",
      "Epoch 30/100\n",
      " - 8s - loss: 2695.2437 - dice_coef: 0.9927\n",
      "\n",
      "Epoch 00030: loss improved from 2734.36597 to 2695.24368, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 31/100\n",
      " - 8s - loss: 2649.7343 - dice_coef: 0.9934\n",
      "\n",
      "Epoch 00031: loss improved from 2695.24368 to 2649.73435, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 32/100\n",
      " - 8s - loss: 2671.4510 - dice_coef: 0.9929\n",
      "\n",
      "Epoch 00032: loss did not improve\n",
      "Epoch 33/100\n",
      " - 8s - loss: 2678.5758 - dice_coef: 0.9932\n",
      "\n",
      "Epoch 00033: loss did not improve\n",
      "Epoch 34/100\n",
      " - 8s - loss: 2826.7296 - dice_coef: 0.9917\n",
      "\n",
      "Epoch 00034: loss did not improve\n",
      "Epoch 35/100\n",
      " - 8s - loss: 2655.3375 - dice_coef: 0.9933\n",
      "\n",
      "Epoch 00035: loss did not improve\n",
      "Epoch 36/100\n",
      " - 8s - loss: 2617.4066 - dice_coef: 0.9937\n",
      "\n",
      "Epoch 00036: loss improved from 2649.73435 to 2617.40663, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 37/100\n",
      " - 8s - loss: 2604.4250 - dice_coef: 0.9936\n",
      "\n",
      "Epoch 00037: loss improved from 2617.40663 to 2604.42499, saving model to ../savedmodels/DECiSION_UNet_brain_softmax_ep100.model\n",
      "Epoch 38/100\n",
      " - 8s - loss: 2658.3273 - dice_coef: 0.9934\n",
      "\n",
      "Epoch 00038: loss did not improve\n",
      "Epoch 39/100\n",
      " - 8s - loss: 2614.9365 - dice_coef: 0.9934\n",
      "\n",
      "Epoch 00039: loss did not improve\n",
      "Epoch 40/100\n",
      " - 8s - loss: 2691.2558 - dice_coef: 0.9928\n",
      "\n",
      "Epoch 00040: loss did not improve\n",
      "Epoch 41/100\n",
      " - 8s - loss: 2896.4529 - dice_coef: 0.9913\n",
      "\n",
      "Epoch 00041: loss did not improve\n",
      "Epoch 42/100\n",
      " - 8s - loss: 2664.0126 - dice_coef: 0.9929\n",
      "\n",
      "Epoch 00042: loss did not improve\n",
      "Epoch 43/100\n",
      " - 8s - loss: 2729.5286 - dice_coef: 0.9928\n",
      "\n",
      "Epoch 00043: loss did not improve\n",
      "\n",
      "\n",
      "Elapsed training time: 5.533333333333333 min\n"
     ]
    }
   ],
   "source": [
    "# Prepare callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(model_path, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='loss', min_delta=0, patience=settings.TRN_EARLY_PATIENCE, verbose=0, mode=\"auto\"),\n",
    "    CSVLogger(csv_path, append=False),\n",
    "    ]\n",
    "\n",
    "\n",
    "# Fit the model using a training set only\n",
    "start_time = time.time()\n",
    "hist = model.fit_generator(gen_train,\n",
    "                 epochs=settings.TRN_NUM_EPOCH,\n",
    "                 steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                 verbose=2,\n",
    "                 shuffle=True,\n",
    "                 callbacks=callbacks)\n",
    "\n",
    "print(\"\\n\\nElapsed training time: {} min\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist,\n",
    "                      settings.TRN_NUM_EPOCH,\n",
    "                      show=False,\n",
    "                      save_path=settings.OUTPUT_PATH + unet.title,\n",
    "                      time_stamp=True,\n",
    "                      metric=\"dice_coef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images and ground truths\n",
    "train_imgs = read_preprocess_image(hdf5_paths[0], settings.HDF5_KEY)\n",
    "train_grndtr = read_preprocess_groundtruth(hdf5_paths[1], settings.HDF5_KEY)\n",
    "\n",
    "# For pipeline testing only, predict on one training image\n",
    "predictions = model.predict(train_imgs[[0]], batch_size=settings.TRN_BATCH_SIZE, verbose=2)\n",
    "\n",
    "# predictions = predictions\n",
    "# predictions = convert_pred_to_img_flatten(predictions, settings.TRN_PRED_THRESHOLD)\n",
    "predictions = convert_pred_to_img(predictions, settings, settings.TRN_PRED_THRESHOLD)\n",
    "\n",
    "\n",
    "show_image(np.squeeze(train_imgs[0]), 'PRED TRAIN org image')\n",
    "show_image(np.squeeze(train_grndtr[0]), 'PRED TRAIN org ground truth')\n",
    "show_image(np.squeeze(predictions[0]), 'PRED TRAIN predicted mask')\n",
    "\n",
    "print(\"  original {} dtype {}\".format(np.max(train_imgs[0]), train_imgs[0].dtype))\n",
    "print(\"  gr truth {} dtype {}\".format(np.max(train_grndtr[0]), train_grndtr[0].dtype))\n",
    "print(\"prediction {} dtype {}\".format(np.max(predictions[0]), predictions[0].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
