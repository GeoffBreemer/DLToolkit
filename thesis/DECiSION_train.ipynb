{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DECiSION incl. data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seeds and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoff/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "from numpy.random import seed\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "import random\n",
    "random.seed = RANDOM_STATE\n",
    "\n",
    "import DECiSION_settings as settings\n",
    "\n",
    "from dltoolkit.utils.generic import model_architecture_to_file, model_summary_to_file, list_images\n",
    "from dltoolkit.nn.segment import UNet_NN\n",
    "from dltoolkit.utils.visual import plot_training_history, plot_roc_curve, plot_precision_recall_curve,\\\n",
    "    print_confusion_matrix, print_classification_report\n",
    "from dltoolkit.iomisc import HDF5Generator_Segment\n",
    "\n",
    "from thesis_common import convert_img_to_pred, convert_pred_to_img, create_hdf5_db,\\\n",
    "    show_image, read_images, read_groundtruths, print_training_info\n",
    "from thesis_metric_loss import dice_coef, weighted_pixelwise_crossentropy_loss\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.optimizers import Adam, SGD, Adadelta\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os, cv2, time, progressbar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change how TensorFlow allocates GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a percentage of the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine how to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_KFOLD_CV = False\n",
    "\n",
    "USE_GENERATORS = True\n",
    "USE_VALIDATION_SET = True\n",
    "\n",
    "if USE_KFOLD_CV:\n",
    "    # For now k-Fold CV only works with all data in memory\n",
    "    USE_GENERATORS = False\n",
    "    USE_VALIDATION_SET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert individual images set to HDF5 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def perform_hdf5_conversion(settings):\n",
    "    \"\"\"Convert the individual .jpg files (images and ground truths) to four .hdf5 files:\n",
    "    - Two with the training set (train_imgs.h5 and train_masks.h5)\n",
    "    - Two with the validation set (val_imgs.h5 and val_masks.h5)\n",
    "    \n",
    "    If TRN_TRAIN_VAL_SPLIT is set to zero the validation set data sets are NOT generated.\n",
    "    \"\"\"\n",
    "    # Prepare the path to the training images and ground truths\n",
    "    img_exts = \".jpg\"\n",
    "    img_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_IMAGES)\n",
    "    msk_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_GROUND_TRUTH)\n",
    "\n",
    "    # Create a list of paths to the individual patient folders\n",
    "    patient_fld_imgs = sorted([os.path.join(img_path, e.name) for e in os.scandir(img_path) if e.is_dir()])\n",
    "    patient_fld_masks = sorted([os.path.join(msk_path, e.name) for e in os.scandir(msk_path) if e.is_dir()])\n",
    "\n",
    "    # Obtain a list of paths to the training images and ground truths for each patient\n",
    "    img_list = []\n",
    "    msk_list = []\n",
    "    for patient_ix, (p_fld_imgs, p_fld_masks) in enumerate(zip(patient_fld_imgs, patient_fld_masks)):\n",
    "        img_list.extend(sorted(list(list_images(basePath=p_fld_imgs,\n",
    "                                                validExts=img_exts)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "        msk_list.extend(sorted(list(list_images(basePath=p_fld_masks,\n",
    "                                                validExts=img_exts)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "\n",
    "    assert(len(img_list) == len(msk_list))\n",
    "\n",
    "    # Split the training set into a training and validation set. Consecutive slices no longer\n",
    "    # belong to the same patient\n",
    "    train_img, val_img, train_msk, val_msk = train_test_split(img_list, msk_list,\n",
    "                                                              test_size=settings.TRN_TRAIN_VAL_SPLIT,\n",
    "                                                              random_state=settings.RANDOM_STATE,\n",
    "                                                              shuffle=True)\n",
    "    \n",
    "    # Create the HDF5 data sets\n",
    "    output_paths = []\n",
    "\n",
    "    # Training images\n",
    "    output_paths.append(create_hdf5_db(train_img, \"train\", img_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings))\n",
    "\n",
    "    # Training ground truths\n",
    "    output_paths.append(create_hdf5_db(train_msk, \"train\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    # Validation images\n",
    "    output_paths.append(create_hdf5_db(val_img, \"val\", img_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings))\n",
    "\n",
    "    # Validation ground truths\n",
    "    output_paths.append(create_hdf5_db(val_msk, \"val\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    return output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converting images to HDF5, full paths:\n",
      "\n",
      "../data/MSC8002/training/train_images.h5\n",
      "../data/MSC8002/training/train_groundtruths.h5\n",
      "../data/MSC8002/training/val_images.h5\n",
      "../data/MSC8002/training/val_groundtruths.h5\n",
      "\n",
      "Created a 0.1 training/validation set\n"
     ]
    }
   ],
   "source": [
    "# Convert image files to HDF5\n",
    "if settings.IS_DEVELOPMENT:\n",
    "    print(\"\\n--- Converting images to HDF5, full paths:\\n\")\n",
    "    hdf5_paths = perform_hdf5_conversion(settings)\n",
    "    \n",
    "    if settings.TRN_TRAIN_VAL_SPLIT == 0:\n",
    "        print(\"\\nValidation set NOT created.\")\n",
    "    else:\n",
    "        print(\"\\nCreated a {} training/validation set\".format(settings.TRN_TRAIN_VAL_SPLIT))\n",
    "else:\n",
    "    # During development avoid performing HDF5 conversion for every run\n",
    "    hdf5_paths = [\"../data/MSC8002/training/train_images.h5\",\n",
    "                  \"../data/MSC8002/training/train_groundtruths.h5\",\n",
    "                  \"../data/MSC8002/training/val_images.h5\",\n",
    "                  \"../data/MSC8002/training/val_groundtruths.h5\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [1.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "class_weights = [settings.CLASS_WEIGHT_BACKGROUND, settings.CLASS_WEIGHT_BLOODVESSEL]\n",
    "print(\"Class distribution: {}\".format(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unet = UNet_NN(img_height=settings.IMG_HEIGHT,\n",
    "               img_width=settings.IMG_WIDTH,\n",
    "               img_channels=settings.IMG_CHANNELS,\n",
    "               num_classes=settings.NUM_CLASSES,\n",
    "               dropout_rate=settings.TRN_DROPOUT_RATE)\n",
    "# model = unet.build_model_BRAIN_3layer(use_bn=True, use_dropout=False)\n",
    "model = unet.build_model_BRAIN_4layer(use_bn=True, use_dropout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare some path strings\n",
    "model_path = os.path.join(settings.MODEL_PATH, \"DECiSION_\" + unet.title + \"_ep{}.model\".format(settings.TRN_NUM_EPOCH))\n",
    "summ_path = os.path.join(settings.OUTPUT_PATH, \"DECiSION_\" + unet.title + \"_model_summary.txt\")\n",
    "csv_path = os.path.join(settings.OUTPUT_PATH, \"DECiSION_\" + unet.title + \"_training_ep{}_bs{}.csv\".format(settings.TRN_NUM_EPOCH,\n",
    "                                                                                            settings.TRN_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/print model architecture information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 240, 240, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 240, 240, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 240, 240, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 240, 240, 32) 9248        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 240, 240, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 120, 120, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 120, 120, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 120, 120, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 120, 120, 64) 36928       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 120, 120, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 60, 60, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 60, 60, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 60, 60, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 60, 60, 128)  147584      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 60, 60, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 30, 30, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 30, 30, 128)  147584      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 30, 30, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 30, 30, 128)  147584      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 30, 30, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 60, 60, 128)  65664       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 60, 256)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 60, 60, 128)  295040      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 60, 60, 128)  147584      conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 120, 120, 64) 32832       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 120, 120, 128 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 120, 120, 64) 73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 120, 120, 64) 36928       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 240, 240, 32) 8224        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 240, 240, 64) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 240, 240, 32) 18464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 240, 240, 32) 9248        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 240, 240, 2)  66          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 240, 240, 2)  0           conv2d_15[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,272,258\n",
      "Trainable params: 1,270,850\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model_summary_to_file(model, summ_path)\n",
    "model_architecture_to_file(unet.model, settings.OUTPUT_PATH + \"DECiSION_\" + unet.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the optimiser, loss function and metrics\n",
    "opt = Adam()\n",
    "# opt = Adadelta()\n",
    "metrics = [dice_coef]\n",
    "loss = weighted_pixelwise_crossentropy_loss(class_weights)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Use generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data generators\n",
      "  Number of training samples: 27\n",
      "Number of validation samples: 3\n",
      " steps_per_epoch: 27.0\n",
      "validation_steps: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare generators, only when NOT using cross validation\n",
    "if USE_GENERATORS:\n",
    "    print(\"Using data generators\")\n",
    "    # Training set generator using optional on-the-fly data augmentation\n",
    "    rdr_train = HDF5Generator_Segment(hdf5_paths[0], hdf5_paths[1],\n",
    "                                      batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                      num_classes=settings.NUM_CLASSES,\n",
    "                                      converter=convert_img_to_pred, \n",
    "                                      feat_key=settings.HDF5_KEY)\n",
    "    gen_train = rdr_train.generator(num_epochs=settings.TRN_NUM_EPOCH)\n",
    "\n",
    "    # Validation set generator (never uses data augmentation)\n",
    "    rdr_val = HDF5Generator_Segment(hdf5_paths[2], hdf5_paths[3],\n",
    "                                    batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                    num_classes=settings.NUM_CLASSES,\n",
    "                                    converter=convert_img_to_pred, \n",
    "                                    feat_key=settings.HDF5_KEY)\n",
    "    gen_val = rdr_val.generator(num_epochs=settings.TRN_NUM_EPOCH)\n",
    "\n",
    "    print(\"  Number of training samples: {}\".format(rdr_train.num_images()))\n",
    "    print(\"Number of validation samples: {}\".format(rdr_val.num_images()))\n",
    "\n",
    "    print(\" steps_per_epoch: {}\".format(rdr_train.num_images()/settings.TRN_BATCH_SIZE))\n",
    "    print(\"validation_steps: {}\".format(rdr_val.num_images()/settings.TRN_BATCH_SIZE))\n",
    "else:\n",
    "    print(\"Not creating generators.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Load all data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATORS:\n",
    "    print(\"Loading all data into memory\")\n",
    "\n",
    "    train_imgs = read_images(hdf5_paths[0], settings.HDF5_KEY)\n",
    "    train_grndtr = read_groundtruths(hdf5_paths[1], settings.HDF5_KEY)\n",
    "    train_grndtr_ext_conv = convert_img_to_pred(train_grndtr, settings.NUM_CLASSES, settings.VERBOSE)  # softmax: 4D\n",
    "    print(\"  Number of training samples: {}\\n\".format(len(train_imgs)))\n",
    "\n",
    "    # Read the validation set (only when NOT using cross-validation)\n",
    "    if not USE_KFOLD_CV:\n",
    "        val_imgs = read_images(hdf5_paths[2], settings.HDF5_KEY)\n",
    "        val_grndtr = read_groundtruths(hdf5_paths[3], settings.HDF5_KEY)\n",
    "        val_grndtr_ext_conv = convert_img_to_pred(val_grndtr, settings.NUM_CLASSES, settings.VERBOSE)  # softmax: 4D\n",
    "        print(\"Number of validation samples: {}\\n\".format(len(val_imgs)))\n",
    "    else:\n",
    "        print(\"Using cross validation, not loading a validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train WITH a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with a training set AND a validation set, using generators\n",
      "\n",
      "Generic information:\n",
      "              Model: UNet_brain_4layer_BN\n",
      "          Saving to: ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "        Input shape: (27, 240, 240, 1)\n",
      "\n",
      "Hyper parameters:\n",
      "          Optimizer: <class 'keras.optimizers.Adam'>\n",
      "                   : 0 = ('lr', 0.0010000000474974513)\n",
      "                   : 1 = ('beta_1', 0.8999999761581421)\n",
      "                   : 2 = ('beta_2', 0.9990000128746033)\n",
      "                   : 3 = ('decay', 0.0)\n",
      "                   : 4 = ('epsilon', 1e-07)\n",
      "                   : 5 = ('amsgrad', False)\n",
      "               Loss: <function weighted_pixelwise_crossentropy_loss.<locals>.loss at 0x1a2179b2f0>\n",
      "         IMG_HEIGHT: 240\n",
      "          IMG_WIDTH: 240\n",
      "       IMG_CHANNELS: 1\n",
      "        NUM_CLASSES: 2\n",
      "        SLICE_START: 59\n",
      "          SLICE_END: 69\n",
      "    IMG_CROP_HEIGHT: 40\n",
      "     IMG_CROP_WIDTH: 40\n",
      "     TRN_BATCH_SIZE: 1\n",
      "  TRN_LEARNING_RATE: 0.001\n",
      "      TRN_NUM_EPOCH: 100\n",
      "TRN_TRAIN_VAL_SPLIT: 0.1\n",
      "   TRN_DROPOUT_RATE: 0.5\n",
      "       TRN_MOMENTUM: 0.99\n",
      " TRN_PRED_THRESHOLD: 0.5\n",
      " TRN_EARLY_PATIENCE: 10\n",
      "      Class weights: [1.0, 8.0]\n",
      "\n",
      "\n",
      "Training start:\n",
      "\n",
      "Epoch 1/100\n",
      " - 26s - loss: 7039.1220 - dice_coef: 0.9479 - val_loss: 3414.1950 - val_dice_coef: 0.9881\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3414.19499, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 2/100\n",
      " - 25s - loss: 2854.2307 - dice_coef: 0.9870 - val_loss: 3195.6402 - val_dice_coef: 0.9929\n",
      "\n",
      "Epoch 00002: val_loss improved from 3414.19499 to 3195.64022, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 3/100\n",
      " - 25s - loss: 2514.0724 - dice_coef: 0.9894 - val_loss: 3026.7271 - val_dice_coef: 0.9934\n",
      "\n",
      "Epoch 00003: val_loss improved from 3195.64022 to 3026.72713, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 4/100\n",
      " - 24s - loss: 2317.7303 - dice_coef: 0.9912 - val_loss: 3090.1248 - val_dice_coef: 0.9939\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      " - 24s - loss: 2195.2972 - dice_coef: 0.9922 - val_loss: 2921.9041 - val_dice_coef: 0.9936\n",
      "\n",
      "Epoch 00005: val_loss improved from 3026.72713 to 2921.90405, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 6/100\n",
      " - 26s - loss: 2152.2311 - dice_coef: 0.9925 - val_loss: 2669.5527 - val_dice_coef: 0.9932\n",
      "\n",
      "Epoch 00006: val_loss improved from 2921.90405 to 2669.55273, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 7/100\n",
      " - 27s - loss: 2243.7128 - dice_coef: 0.9915 - val_loss: 7982.7798 - val_dice_coef: 0.9949\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/100\n",
      " - 28s - loss: 2327.0942 - dice_coef: 0.9910 - val_loss: 2728.8878 - val_dice_coef: 0.9916\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      " - 28s - loss: 2251.6423 - dice_coef: 0.9918 - val_loss: 2584.2461 - val_dice_coef: 0.9930\n",
      "\n",
      "Epoch 00009: val_loss improved from 2669.55273 to 2584.24609, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 10/100\n",
      " - 28s - loss: 2183.2311 - dice_coef: 0.9920 - val_loss: 5656.2542 - val_dice_coef: 0.9954\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      " - 28s - loss: 2076.8055 - dice_coef: 0.9930 - val_loss: 7282.5312 - val_dice_coef: 0.9954\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      " - 27s - loss: 2077.3332 - dice_coef: 0.9929 - val_loss: 7172.3929 - val_dice_coef: 0.9951\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      " - 27s - loss: 2168.1821 - dice_coef: 0.9921 - val_loss: 4454.6376 - val_dice_coef: 0.9947\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      " - 27s - loss: 2195.8766 - dice_coef: 0.9919 - val_loss: 8177.7586 - val_dice_coef: 0.9951\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      " - 27s - loss: 2138.9732 - dice_coef: 0.9926 - val_loss: 5499.2573 - val_dice_coef: 0.9942\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      " - 30s - loss: 2088.5037 - dice_coef: 0.9929 - val_loss: 5817.4181 - val_dice_coef: 0.9940\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      " - 30s - loss: 2025.5468 - dice_coef: 0.9935 - val_loss: 2652.9530 - val_dice_coef: 0.9941\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      " - 31s - loss: 1973.6166 - dice_coef: 0.9940 - val_loss: 2556.5066 - val_dice_coef: 0.9943\n",
      "\n",
      "Epoch 00018: val_loss improved from 2584.24609 to 2556.50659, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 19/100\n",
      " - 32s - loss: 1968.1875 - dice_coef: 0.9940 - val_loss: 2878.0743 - val_dice_coef: 0.9947\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/100\n",
      " - 36s - loss: 1936.5405 - dice_coef: 0.9943 - val_loss: 3710.4449 - val_dice_coef: 0.9944\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/100\n",
      " - 45s - loss: 1947.8409 - dice_coef: 0.9941 - val_loss: 2707.4525 - val_dice_coef: 0.9931\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      " - 44s - loss: 1939.9302 - dice_coef: 0.9942 - val_loss: 2622.3686 - val_dice_coef: 0.9942\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      " - 36s - loss: 1301.9982 - dice_coef: 0.9949 - val_loss: 3308.2510 - val_dice_coef: 0.9967\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      " - 33s - loss: 937.8147 - dice_coef: 0.9948 - val_loss: 1605.1540 - val_dice_coef: 0.9961\n",
      "\n",
      "Epoch 00024: val_loss improved from 2556.50659 to 1605.15401, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 25/100\n",
      " - 37s - loss: 729.2480 - dice_coef: 0.9961 - val_loss: 1355.4853 - val_dice_coef: 0.9963\n",
      "\n",
      "Epoch 00025: val_loss improved from 1605.15401 to 1355.48527, saving model to ../savedmodels/DECiSION_UNet_brain_4layer_BN_ep100.model\n",
      "Epoch 26/100\n",
      " - 41s - loss: 654.4397 - dice_coef: 0.9964 - val_loss: 1797.4167 - val_dice_coef: 0.9970\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      " - 45s - loss: 736.2859 - dice_coef: 0.9959 - val_loss: 7197.0518 - val_dice_coef: 0.9961\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      " - 51s - loss: 945.8013 - dice_coef: 0.9948 - val_loss: 1839.8304 - val_dice_coef: 0.9954\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      " - 55s - loss: 704.3414 - dice_coef: 0.9959 - val_loss: 7128.7725 - val_dice_coef: 0.9961\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      " - 33s - loss: 682.6934 - dice_coef: 0.9963 - val_loss: 1421.2547 - val_dice_coef: 0.9947\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      " - 32s - loss: 560.0716 - dice_coef: 0.9968 - val_loss: 3862.5884 - val_dice_coef: 0.9969\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/100\n",
      " - 39s - loss: 476.4951 - dice_coef: 0.9974 - val_loss: 1780.0542 - val_dice_coef: 0.9968\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n"
     ]
    }
   ],
   "source": [
    "if USE_GENERATORS and USE_VALIDATION_SET:\n",
    "    # Prepare callbacks\n",
    "    callbacks = [ModelCheckpoint(model_path, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "                 EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0,\n",
    "                               patience=settings.TRN_EARLY_PATIENCE,\n",
    "                               verbose=0,\n",
    "                               mode=\"auto\"),\n",
    "                 CSVLogger(csv_path, append=False),\n",
    "                 ]\n",
    "\n",
    "    print(\"Training with a validation set, using generators.\")\n",
    "    print_training_info(unet, model_path, rdr_train.img_shape, settings, class_weights, opt, loss)\n",
    "    print(\"Training start:\\n\")\n",
    "\n",
    "    # Fit the model using generators and a validation set\n",
    "    start_time = time.time()\n",
    "    hist = model.fit_generator(gen_train,\n",
    "                     epochs=settings.TRN_NUM_EPOCH,\n",
    "                     steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                     verbose=2,\n",
    "                     validation_data=gen_val,\n",
    "                     validation_steps=rdr_val.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                     shuffle=True,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "    print(\"\\n\\nElapsed training time: {} min\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train WITHOUT a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GENERATORS and not USE_VALIDATION_SET:\n",
    "    # Prepare callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "        EarlyStopping(monitor='loss', min_delta=0, patience=settings.TRN_EARLY_PATIENCE, verbose=0, mode=\"auto\"),\n",
    "        CSVLogger(csv_path, append=False),\n",
    "        ]\n",
    "\n",
    "    print(\"Training without a validation set, using generators.\")\n",
    "    print_training_info(unet, model_path, rdr_train.img_shape, settings, class_weights, opt, loss)\n",
    "    print(\"Training start:\\n\")\n",
    "\n",
    "    # Fit the model using a training set only\n",
    "    start_time = time.time()\n",
    "    hist = model.fit_generator(gen_train,\n",
    "                     epochs=settings.TRN_NUM_EPOCH,\n",
    "    #                  epochs=3,\n",
    "                     steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                     verbose=2,\n",
    "                     shuffle=True,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "    print(\"\\n\\nElapsed training time: {} min\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with all data loaded in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train WITH a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATORS and USE_VALIDATION_SET:\n",
    "    print(\"Training with a validation set, using generators.\")\n",
    "    print_training_info(unet, model_path, train_imgs.shape, settings, class_weights, opt, loss)\n",
    "    print(\"Training start:\\n\")\n",
    "\n",
    "    # Prepare callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "        EarlyStopping(monitor='val_loss', min_delta=0, patience=settings.TRN_EARLY_PATIENCE, verbose=0, mode=\"auto\"),\n",
    "        CSVLogger(csv_path, append=False),\n",
    "        ]\n",
    "\n",
    "    # Fit the model using a training set only\n",
    "    start_time = time.time()\n",
    "    hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                     epochs=settings.TRN_NUM_EPOCH,\n",
    "                     batch_size=settings.TRN_BATCH_SIZE,\n",
    "                     verbose=1,\n",
    "                     shuffle=True,\n",
    "                     validation_data=(val_imgs, val_grndtr_ext_conv),\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "    print(\"\\n\\nElapsed training time: {} min\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train WITHOUT a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATORS and not USE_VALIDATION_SET:\n",
    "    print(\"Training without a validation set, all data in memory.\")\n",
    "    print_training_info(unet, model_path, train_imgs.shape, settings, class_weights, opt, loss)\n",
    "    print(\"Training start:\\n\")\n",
    "\n",
    "    # Prepare callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "        EarlyStopping(monitor='loss', min_delta=0, patience=settings.TRN_EARLY_PATIENCE, verbose=0, mode=\"auto\"),\n",
    "        CSVLogger(csv_path, append=False),\n",
    "        ]\n",
    "\n",
    "    # Fit the model using a training set only\n",
    "    start_time = time.time()\n",
    "    hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                     epochs=settings.TRN_NUM_EPOCH,\n",
    "                     batch_size=settings.TRN_BATCH_SIZE,\n",
    "                     verbose=1,\n",
    "                     shuffle=True,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "    print(\"\\n\\nElapsed training time: {} min\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
    "\n",
    "https://github.com/keras-team/keras/issues/4530\n",
    "\n",
    "https://stackoverflow.com/questions/40854232/keras-scikit-learn-using-fit-generator-with-cross-validation/40866543#40866543\n",
    "\n",
    "https://stackoverflow.com/questions/41214527/k-fold-cross-validation-using-keras\n",
    "\n",
    "https://www.kaggle.com/stefanie04736/simple-keras-model-with-k-fold-cross-validation\n",
    "\n",
    "NICE: https://www.kaggle.com/zfturbo/fishy-keras-lb-1-25267/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if USE_KFOLD_CV:\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    print(\"Training using cross validation\")\n",
    "    print_training_info(unet, model_path, train_imgs.shape, settings, class_weights, opt, loss)\n",
    "    print(\"Training start:\\n\")\n",
    "\n",
    "    # Set the optimiser, loss function and metrics\n",
    "    opt = Adam()\n",
    "    metrics = [dice_coef]\n",
    "    loss = weighted_pixelwise_crossentropy_loss(class_weights)\n",
    "\n",
    "    # Create the folds\n",
    "    kfold = KFold(n_splits=settings.TRN_NUM_KFOLD_SPLITS, shuffle=True, random_state=settings.RANDOM_STATE)\n",
    "    cvscores = []\n",
    "    cvlosses = []\n",
    "    fold_id = 1\n",
    "    for train_idx, test_idx in kfold.split(train_imgs, train_grndtr_ext_conv):\n",
    "        print(\"\\n-----> Fold {} of {} <-----\".format(fold_id, settings.TRN_NUM_KFOLD_SPLITS))\n",
    "        print(\"Training: {} slices, test: {} slices\".format(len(train_idx), len(test_idx)))\n",
    "        \n",
    "        # Create and compile the model before each fold (to reset the weights)\n",
    "        model = unet.build_model_softmax()\n",
    "        model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "\n",
    "        # Include the number of the fold in the file name\n",
    "        kfold_path = os.path.join(settings.MODEL_PATH, \"DECiSION_\" + unet.title + \"_ep{}_kfold{}.model\".format(settings.TRN_NUM_EPOCH, fold_id))\n",
    "\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(kfold_path, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=settings.TRN_EARLY_PATIENCE, verbose=0, mode=\"auto\"),\n",
    "            CSVLogger(csv_path, append=False),\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(train_imgs[train_idx], train_grndtr_ext_conv[train_idx],\n",
    "                         epochs=settings.TRN_NUM_EPOCH,\n",
    "                         batch_size=settings.TRN_BATCH_SIZE,\n",
    "                         verbose=2,\n",
    "                         shuffle=True,\n",
    "                         callbacks=callbacks)\n",
    "\n",
    "        scores = model.evaluate(train_imgs[test_idx], train_grndtr_ext_conv[test_idx], verbose=2)\n",
    "        print(\"\\n-----> Fold {}: {}: {:.2f}%% - {}: {:.2f}\\n\".format(fold_id,\n",
    "                                                          model.metrics_names[1],\n",
    "                                                          scores[1]*100,\n",
    "                                                          model.metrics_names[0],\n",
    "                                                          scores[0]))\n",
    "        cvscores.append(scores[1] * 100)\n",
    "        cvlosses.append(scores[0])\n",
    "\n",
    "        fold_id += 1\n",
    "\n",
    "    print(\"Results\")\n",
    "    print(\"------------------\")\n",
    "    for i in range(NUM_SPLITS):\n",
    "        print(\"Fold {}: {}: {:.2f}% - {}: {:.2f}\\n\".format(i,\n",
    "                                                          model.metrics_names[1],\n",
    "                                                          cvscores[i],\n",
    "                                                          model.metrics_names[0],\n",
    "                                                          cvlosses[i]))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    print(\"\\nEstimated test performance: mean and std dev\")\n",
    "    print(\"------------------\")\n",
    "    print(\"\\n{}: {:.2f} (+/- {:.2f})\".format(model.metrics_names[0], np.mean(cvlosses), np.std(cvlosses)))\n",
    "    print(\"{}: {:.2f}% (+/- {:.2f}%)\".format(model.metrics_names[1], np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "    print(\"\\nFold Id with the highest DICE: {}\".format(np.argmax(cvscores)+1))\n",
    "    print(\" Fold Id with the lowest loss: {}\".format(np.argmin(cvscores)+1))\n",
    "    \n",
    "    print(\"\\nFitting model using all training data\")\n",
    "    print(\"------------------\")\n",
    "    # Prepare callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "        EarlyStopping(monitor='loss', min_delta=0, patience=settings.TRN_EARLY_PATIENCE, verbose=0, mode=\"auto\"),\n",
    "        CSVLogger(csv_path, append=False),\n",
    "        ]\n",
    "\n",
    "    model = unet.build_model_softmax()\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "\n",
    "    # Fit the model using a training set only\n",
    "    start_time = time.time()\n",
    "    hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "#                      epochs=settings.TRN_NUM_EPOCH,\n",
    "                     epochs=2,\n",
    "                     batch_size=settings.TRN_BATCH_SIZE,\n",
    "                     verbose=1,\n",
    "                     shuffle=True,\n",
    "                     callbacks=callbacks)\n",
    "\n",
    "    print(\"\\n\\nElapsed training time: {} min\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot/save the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not USE_KFOLD_CV:\n",
    "    plot_training_history(hist,\n",
    "                          settings.TRN_NUM_EPOCH,\n",
    "                          show=True,\n",
    "                          save_path=settings.OUTPUT_PATH + unet.title,\n",
    "                          time_stamp=True,\n",
    "                          metric=\"dice_coef\")\n",
    "else:\n",
    "    print(\"Using cross-validation, no training history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read training images and ground truths\n",
    "train_imgs = read_images(hdf5_paths[0], settings.HDF5_KEY)\n",
    "train_grndtr = read_groundtruths(hdf5_paths[1], settings.HDF5_KEY)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(train_imgs, batch_size=settings.TRN_BATCH_SIZE, verbose=2)\n",
    "print(\"Predictions shape: {}\".format(predictions.shape))\n",
    "\n",
    "# Convert predictions to images\n",
    "predictions_imgs = convert_pred_to_img(predictions,\n",
    "                                       threshold=settings.TRN_PRED_THRESHOLD,\n",
    "                                       verbose=settings.VERBOSE)\n",
    "\n",
    "# Show a single image, ground truth and segmentation map\n",
    "show_image(np.squeeze(predictions_imgs[0]), 'Segmentation map')\n",
    "show_image(np.squeeze(train_grndtr[0]), 'Ground truth')\n",
    "show_image(np.squeeze(train_imgs[0]), 'Original image')\n",
    "\n",
    "print(\"   seg map {} dtype {}\".format(np.max(predictions_imgs[0]), predictions_imgs[0].dtype))\n",
    "print(\"  gr truth {} dtype {}\".format(np.max(train_grndtr[0]), train_grndtr[0].dtype))\n",
    "print(\"  original {} dtype {}\".format(np.max(train_imgs[0]), train_imgs[0].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
