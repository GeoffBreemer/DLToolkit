{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VOLVuLuS model\n",
    "Train the VOLumetric VesseL Segmentation (VOLVuLuS) model, a variant of the 3D U-Net by ([Çiçek 2016](#References)). The training set comprises **volumes of** (as opposed to **individual** images used by DECiSION) 320 by 320 pixels grayscale axial MRI images along with ground truth images highlighting blood vessels associated with each MRI image. The trained model can be applied to (unseen) MRI images to produce segmentation maps using `VOLVuLuS_test.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seeds and import packages\n",
    "Setting the seeds first is meant to achieve reproducibility, though this goal is not entirely achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoff/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "from numpy.random import seed\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "import random\n",
    "random.seed = RANDOM_STATE\n",
    "\n",
    "# Model and training settings\n",
    "import VOLVuLuS_settings as settings\n",
    "\n",
    "# Toolkit imports\n",
    "from dltoolkit.utils.generic import model_architecture_to_file, model_summary_to_file, list_images\n",
    "from dltoolkit.nn.segment import UNet_3D_NN\n",
    "from dltoolkit.utils.visual import plot_training_history\n",
    "\n",
    "from thesis_common import convert_img_to_pred_3d, convert_pred_to_img_3d, create_hdf5_db_3d,\\\n",
    "    show_image, print_training_info, read_groundtruths, read_images, load_training_3d\n",
    "from thesis_metric_loss import dice_coef_threshold, weighted_pixelwise_crossentropy_loss\n",
    "\n",
    "# Keras imports\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, TensorBoard, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import os, cv2, time, progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change how TensorFlow allocates GPU memory\n",
    "Setting `gpu_options.allow_growth` to `True` means TensorFlow will allocate GPU memory as needed rather than using all available memory from the start. This enables monitoring of actual memory usage and determining how close the notebook gets to running out of memory. This has no effect on non-GPU machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a percentage of the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine training settings\n",
    "The variables below determine how the model will be trained:\n",
    "\n",
    "- `USE_VALIDATION_SET`: set to `True` to use a validation set during training, which will be the case most of the time. set to `False` to not use a validation set, e.g. during pipeline development/validation.\n",
    "\n",
    "Contrary to DECiSION, VOLVuLuS always uses data generators due to the size of the model.\n",
    "\n",
    "**Note** if `TRN_TRAIN_VAL_SPLIT` is set to 0, a validation set will *not* be created, even if `USE_VALIDATION_SET` is set to `True`. In fact, `USE_VALIDATION_SET` will be set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_VALIDATION_SET = True\n",
    "\n",
    "if not USE_VALIDATION_SET:\n",
    "    # If no validation set is to be used override the split value\n",
    "    settings.TRN_TRAIN_VAL_SPLIT = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Contrary to DECiSION, data used to train VOLVuLuS is not converted to HDF5 format. Also, all data is read into memory; generators are not used. This was done to simplify the code to assist with debugging when it became apparent that training the model required huge amounts of memory. Keeping code as clean as possible ruled out issues with our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading images  33% |##############                             | ETA:  0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading images 100% |###########################################| Time: 0:00:00\n",
      "Reading images 100% |###########################################| Time: 0:00:00\n",
      "Reading images N/A% |                                          | ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training ground truths\n",
      "Elapsed time: 0.11s\n",
      "Loading validation images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading images 100% |###########################################| Time: 0:00:00\n",
      "Reading images 100% |###########################################| Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation ground truths\n",
      "Elapsed time: 0.03s\n"
     ]
    }
   ],
   "source": [
    "train_imgs, train_grndtr, train_grndtr_ext_conv, val_imgs, val_grndtr, val_grndtr_ext_conv, num_patients = load_training_3d(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the class distribution\n",
    "Assigning a higher weight to the positive class (i.e. blood vessels) means the model will pay \"more attention\" to that class. This is useful in the current class imbalance scenario because the number of background (i.e. non-blood vessel) pixels far exceed the number of blood vessel pixels. Without setting a different class weight for the blood vessel class the model would simply assign the background class to all pixels to achieve a low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [1.0, 80.0]\n"
     ]
    }
   ],
   "source": [
    "class_weights = [settings.CLASS_WEIGHT_BACKGROUND, settings.CLASS_WEIGHT_BLOODVESSEL]\n",
    "print(\"Class distribution: {}\".format(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the 3D U-Net model\n",
    "Instantiate the 3D U-Net model. Use different versions of the `build_model_XXX()` function to try different variations of the model. **Warning**: Changing the model and/or its parameters will change the name of the file the trained model will be saved to. Make sure to update the `VOLVuLuS_test_ipynb` notebook accordingly to ensure it uses the correct saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet_3D_NN(img_height=settings.IMG_HEIGHT,\n",
    "                  img_width=settings.IMG_WIDTH,\n",
    "                  num_slices=settings.SLICE_END - settings.SLICE_START,\n",
    "                  img_channels=settings.IMG_CHANNELS,\n",
    "                  num_classes=settings.NUM_CLASSES)\n",
    "\n",
    "model = unet.build_model_alt(num_layers=settings.MDL_LAYERS,\n",
    "                             n_base_filters=settings.MDL_BASE_FLTRS,\n",
    "                             deconvolution=settings.MDL_DECON,\n",
    "                             use_bn=settings.MDL_BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create paths\n",
    "This cell just creates a few paths used later to save training output (e.g. the model architecture, training results and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"VOLVuLuS_\" + unet.title + \"_W\"+ str(settings.CLASS_WEIGHT_BLOODVESSEL) + \"_\" + settings.TRN_LOSS + \"_BS\" + \"{:03}\".format(settings.TRN_BATCH_SIZE)\n",
    "\n",
    "model_path = os.path.join(settings.MODEL_PATH, prefix + \".model\")\n",
    "summ_path = os.path.join(settings.OUTPUT_PATH, prefix + \"_model_summary.txt\")\n",
    "csv_path = os.path.join(settings.OUTPUT_PATH, prefix + \"_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/print model architecture information\n",
    "Save the model's architecture to a file, print it in the cell below and save a diagram to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 32, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 256, 256, 32, 448         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 32, 64          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 256, 256, 32, 13856       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 256, 32, 128         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 128, 128, 16, 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 128, 128, 16, 27680       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 16, 128         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 128, 128, 16, 55360       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 16, 256         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTrans (None, 256, 256, 32, 32832       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 32, 0           conv3d_transpose_1[0][0]         \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 256, 256, 32, 82976       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256, 256, 32, 128         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 256, 256, 32, 27680       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256, 256, 32, 128         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 256, 256, 32, 66          batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 241,730\n",
      "Trainable params: 241,314\n",
      "Non-trainable params: 416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model_summary_to_file(model, summ_path)\n",
    "model_architecture_to_file(unet.model, os.path.join(settings.OUTPUT_PATH, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "Set the loss function, optimiser and metric and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimiser, loss function and metrics\n",
    "if settings.TRN_LOSS == \"ADAM\":\n",
    "    opt = Adam(lr=settings.TRN_LEARNING_RATE, amsgrad=settings.TRN_AMS_GRAD)\n",
    "else:\n",
    "    opt = SGD(lr=settings.TRN_LEARNING_RATE)\n",
    "\n",
    "# Softmax:\n",
    "metrics = [dice_coef_threshold(settings.TRN_PRED_THRESHOLD)]\n",
    "loss = weighted_pixelwise_crossentropy_loss(class_weights)\n",
    "\n",
    "# Sigmoid:\n",
    "# metrics = [dice_coef]\n",
    "# loss = \"binary_crossentropy\"\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare callbacks\n",
    "Prepare callbacks used during training:\n",
    "\n",
    "- TensorBoard: basic TensorBoard visualizations (not always used)\n",
    "- EarlyStopping: Stop training when a monitored quantity has stopped improving\n",
    "- CSVLogger: streams epoch results to a csv file\n",
    "- ModelCheckpoint: save the model after every epoch\n",
    "- ReduceLROPlateau: reduce the learning when progress halts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_VALIDATION_SET:\n",
    "    loss_str = \"val_loss\"\n",
    "else:\n",
    "    loss_str = \"loss\"\n",
    "\n",
    "# tb_callb = TensorBoard(log_dir=settings.OUTPUT_PATH + unet.title,\n",
    "#                        write_graph=True,\n",
    "#                        batch_size=settings.TRN_BATCH_SIZE)\n",
    "\n",
    "cvs_callb = CSVLogger(csv_path, append=False)\n",
    "\n",
    "\n",
    "red_callb = ReduceLROnPlateau(monitor=loss_str,\n",
    "                          factor=settings.TRN_PLAT_FACTOR,\n",
    "                          patience=settings.TRN_PLAT_PATIENCE,\n",
    "                          verbose=1,\n",
    "                          mode=\"min\")\n",
    "\n",
    "mc_callb = ModelCheckpoint(model_path,\n",
    "                           monitor=loss_str,\n",
    "                           mode=\"min\",\n",
    "                           save_best_only=True,\n",
    "                           save_weights_only=True,\n",
    "                           verbose=1)\n",
    "\n",
    "es_callb = EarlyStopping(monitor=loss_str,\n",
    "                         min_delta=0,\n",
    "                         patience=settings.TRN_EARLY_PATIENCE,\n",
    "                         verbose=0,\n",
    "                         mode=\"auto\")\n",
    "\n",
    "callbacks = [mc_callb, es_callb, cvs_callb, red_callb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Execute the training process. All data is loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if USE_VALIDATION_SET:\n",
    "    print(\"Training with a validation set, using all data in memory.\")\n",
    "    print_training_info(unet, model_path, train_imgs.shape, val_imgs.shape,\n",
    "                        settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "    # Fit the model using generators and a validation set\n",
    "    hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                     epochs=settings.TRN_NUM_EPOCH,\n",
    "                     batch_size=settings.TRN_BATCH_SIZE,\n",
    "                     verbose=2,\n",
    "                     shuffle=True,\n",
    "                     validation_data=(val_imgs, val_grndtr_ext_conv),\n",
    "                     callbacks=callbacks\n",
    "                    )\n",
    "\n",
    "else:\n",
    "    print(\"Training without a validation set, using all data in memory.\")\n",
    "    print_training_info(unet, model_path, train_imgs.shape, None,\n",
    "                        settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "    # Fit the model using a training set only\n",
    "    start_time = time.time()\n",
    "    hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                     epochs=settings.TRN_NUM_EPOCH,\n",
    "                     batch_size=settings.TRN_BATCH_SIZE,\n",
    "                     verbose=2,\n",
    "                     shuffle=True,\n",
    "                     callbacks=callbacks\n",
    "                    )\n",
    "\n",
    "print(\"\\n\\nElapsed training time: {:.2f} min.\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform pipeline test\n",
    "Use the trained model on one sample in the training data set. This is just to perform pipeline testing during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENT_ID = 0\n",
    "IX = 0\n",
    "\n",
    "# For pipeline testing only\n",
    "predictions = model.predict(train_imgs, batch_size=settings.TRN_BATCH_SIZE, verbose=2)\n",
    "\n",
    "# Transpose images and ground truths to the correct oder\n",
    "train_imgs_tr = np.transpose(train_imgs, axes=(0, 3, 1, 2, 4))\n",
    "train_grndtr_tr = np.transpose(train_grndtr, axes=(0, 3, 1, 2, 4))\n",
    "\n",
    "# predictions = predictions\n",
    "predictions_imgs = convert_pred_to_img_3d(predictions,\n",
    "                                       threshold=settings.TRN_PRED_THRESHOLD,\n",
    "                                       verbose=settings.VERBOSE)\n",
    "\n",
    "show_image(np.squeeze(train_imgs_tr[0, 1]), 'PRED TRAIN org image')\n",
    "show_image(np.squeeze(train_grndtr_tr[0, 1]), 'PRED TRAIN org ground truth')\n",
    "show_image(np.squeeze(predictions_imgs[0, 1]), 'PRED TRAIN predicted mask')\n",
    "\n",
    "print(\"  original {} dtype {}\".format(np.max(train_imgs_tr[PATIENT_ID,IX]),\n",
    "                                      train_imgs_tr[PATIENT_ID,IX].dtype))\n",
    "print(\"  gr truth {} dtype {}\".format(np.max(train_grndtr_tr[PATIENT_ID,IX]),\n",
    "                                      train_grndtr_tr[PATIENT_ID,IX].dtype))\n",
    "print(\"prediction {} dtype {}\".format(np.max(predictions_imgs[PATIENT_ID,IX]),\n",
    "                                      predictions_imgs[PATIENT_ID,IX].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot/save the training results\n",
    "Show a plot of the training loss and Dice coefficient by epoch and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist,\n",
    "                      show=True,\n",
    "                      save_path=os.path.join(settings.OUTPUT_PATH, prefix),\n",
    "                      time_stamp=True,\n",
    "                      metric=\"dice_coef_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training complete\n",
    "The trained model is now ready to be applied to test MRI images using `VOLuLuS_test.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Cicek]*: Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: learning dense volumetric segmentation from sparse annotation. In *International Conference on Medical Image Computing and Computer- Assisted Intervention*, pages 424–432. Springer, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
