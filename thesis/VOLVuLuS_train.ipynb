{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VOLVuLuS\n",
    "Train the VOLumetric VesseL Segmentation (VOLVuLuS) model, a variant of the 3D U-Net by ([Çiçek 2016](#References)). The training set comprises **volumes of** (as opposed to **individual** images used by DECiSION) 320 by 320 pixels grayscale axial MRI images along with ground truth images highlighting blood vessels associated with each MRI image. The trained model can be applied to (unseen) MRI images to produce segmentation maps using `DECiSION_test.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seeds and import packages\n",
    "Setting the seeds first is meant to achieve reproducibility, though this goal is not entirely achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "from numpy.random import seed\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "import random\n",
    "random.seed = RANDOM_STATE\n",
    "\n",
    "# Model and training settings\n",
    "import VOLVuLuS_settings as settings\n",
    "\n",
    "# Toolkit imports\n",
    "from dltoolkit.utils.generic import model_architecture_to_file, model_summary_to_file, list_images\n",
    "from dltoolkit.nn.segment import UNet_3D_NN\n",
    "from dltoolkit.utils.visual import plot_training_history, plot_roc_curve, plot_precision_recall_curve,\\\n",
    "    print_confusion_matrix, print_classification_report\n",
    "from dltoolkit.iomisc import HDF5Generator_Segment\n",
    "\n",
    "from thesis_common import convert_img_to_pred_3d, convert_pred_to_img_3d, create_hdf5_db_3d,\\\n",
    "    show_image, read_images, read_groundtruths, print_training_info\n",
    "from thesis_metric_loss import dice_coef, weighted_pixelwise_crossentropy_loss\n",
    "\n",
    "# Keras imports\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import os, cv2, time, progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change how TensorFlow allocates GPU memory\n",
    "Setting `gpu_options.allow_growth` to `True` means TensorFlow will allocate GPU memory as needed rather than using all available memory from the start. This enables monitoring of actual memory usage and determining how close the notebook gets to running out of memory. This has no effect on non-GPU machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a percentage of the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine training settings\n",
    "The variables below determine how the model will be trained:\n",
    "\n",
    "- `USE_VALIDATION_SET`: set to `True` to use a validation set during training, which will be the case most of the time. set to `False` to not use a validation set, e.g. during pipeline development/validation.\n",
    "- `IS_DEVELOPMENT`: set to `True` to re-create the HDF5 files every time training is executed, `False` otherwise.\n",
    "\n",
    "Contrary to DECiSION, VOLVuLuS always uses data generators due to the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_VALIDATION_SET = False\n",
    "IS_DEVELOPMENT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert individual JPGs to HDF5 data sets\n",
    "This function converts individual JPG files to HDF5 data sets. It creates up to four HDF5 files depending on whether a portion of the files is set aside as a validation set using the `settings.TRN_TRAIN_VAL_SPLIT` parameter.\n",
    "\n",
    "This conversion only has to be performed once rather than prior to every training run. However, it takes very little time to run. **Note**: if any changes are made to image pre-processing applied to MRI images and/or their ground truths, the conversion MUST be executed prior to the next training run. Failing to do this means training will be performed on existing HDF5 data sets that do not include the revised pre-processing.\n",
    "\n",
    "The folder stucture is shown below. MRI images (i.e. the individial JPG files, one for each *slice*) are located in the `images` folder. Each patient has a separate subfolder. Ground truths are located in the `groundtruths` folder, again each patient has a separate subfolder. The association between MRI image and ground truth is based on the name of the patient's subfolder **and** the alphabetically sorted file name. An entire volume of MRI images/ground truths pair constitutes one training sample. The resulting HDF5 data sets are stored in the `images` and `ground truths` subfolders. The training data sets are called `train_images.h5` and `train_groundtruths.h5`. The validation sets, if they were created, are called `val_images.h5` and `val_groundtruths.h5`.\n",
    "\n",
    "**Note**: VOLVuLuS (`training3d`) and DECiSION (`training`) use different folders to store training data. \n",
    "\n",
    "```\n",
    "---training3d\n",
    "|     |--- images\n",
    "|     |      |---patient1\n",
    "|     |      |      |--- image1\n",
    "|     |      |      |--- image2\n",
    "|     |      |      ...\n",
    "|     |      |      |--- imageM\n",
    "|     |      |---patient2\n",
    "|     |      ...\n",
    "|     |      |---patientN\n",
    "|     |--- groundtruths\n",
    "|     |      |---patient1\n",
    "|     |      |      |--- groundtruth1\n",
    "|     |      |      |--- groundtruth2\n",
    "|     |      |      ...\n",
    "|     |      |      |--- groundtruthM\n",
    "|     |      |---patient2\n",
    "|     |      ...\n",
    "|     |      |---patientN\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdf5_conversion_3d(settings):\n",
    "    \"\"\"Convert the training and test images, ground truths and masks to HDF5 format. The assumption is that images\n",
    "    are all placed in the same folder, regardless of the patient.\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_IMAGES)\n",
    "    msk_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_GROUND_TRUTH)\n",
    "\n",
    "    # Create a list of paths to the individual patient folders\n",
    "    patient_fld_imgs = sorted([os.path.join(img_path, e.name) for e in os.scandir(img_path) if e.is_dir()])\n",
    "    patient_fld_masks = sorted([os.path.join(msk_path, e.name) for e in os.scandir(msk_path) if e.is_dir()])\n",
    "\n",
    "    # Split the training set into a training and validation set.\n",
    "    train_img, val_img, train_msk, val_msk = train_test_split(patient_fld_imgs, patient_fld_masks,\n",
    "                                                              test_size=settings.TRN_TRAIN_VAL_SPLIT,\n",
    "                                                              random_state=settings.RANDOM_STATE,\n",
    "                                                              shuffle=True)\n",
    "\n",
    "    output_paths = []\n",
    "\n",
    "    # Convert training images\n",
    "    output_paths.append(create_hdf5_db_3d(train_img, \"train3d\", img_path,\n",
    "                                        (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                        img_exts=settings.IMG_EXTENSION, key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                        settings=settings))\n",
    "\n",
    "    # Convert training ground truths\n",
    "    output_paths.append(create_hdf5_db_3d(train_msk, \"train3d\", msk_path,\n",
    "                                        (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                        img_exts=settings.IMG_EXTENSION, key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                        settings=settings, is_mask=True))\n",
    "\n",
    "    # Convert validation images\n",
    "    output_paths.append(create_hdf5_db_3d(val_img, \"val3d\", img_path,\n",
    "                                        (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                        img_exts=settings.IMG_EXTENSION, key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                        settings=settings))\n",
    "\n",
    "    # Convert validation ground truths\n",
    "    output_paths.append(create_hdf5_db_3d(val_msk, \"val3d\", msk_path,\n",
    "                                        (settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS),\n",
    "                                        img_exts=settings.IMG_EXTENSION,\n",
    "                                        key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                        settings=settings, is_mask=True))\n",
    "\n",
    "    return output_paths, len(patient_fld_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converted images to HDF5, full paths:\n",
      "\n",
      "../data/MSC8002/training3d/train3d_images.h5\n",
      "reading patient folders:  ['../data/MSC8002/training3d/images/patient_1']\n",
      "---\n",
      "../data/MSC8002/training3d/train3d_groundtruths.h5\n",
      "reading patient folders:  ['../data/MSC8002/training3d/groundtruths/patient_1']\n",
      "---\n",
      "No patient subfolders found, not creating HDF5 file\n",
      "No patient subfolders found, not creating HDF5 file\n",
      "\n",
      "Validation set NOT created.\n"
     ]
    }
   ],
   "source": [
    "if IS_DEVELOPMENT:\n",
    "    print(\"\\n--- Converted images to HDF5, full paths:\\n\")\n",
    "    hdf5_paths, num_patients = perform_hdf5_conversion_3d(settings)\n",
    "    \n",
    "    if settings.TRN_TRAIN_VAL_SPLIT == 0:\n",
    "        print(\"\\nValidation set NOT created.\")\n",
    "        USE_VALIDATION_SET = False\n",
    "    else:\n",
    "        print(\"\\nCreated a {:.2f} training/validation set split.\".format(settings.TRN_TRAIN_VAL_SPLIT))\n",
    "else:\n",
    "    # Use existing HDF5 data sets\n",
    "    hdf5_paths = [\"../data/MSC8002/training3d/images.h5\",\n",
    "                  \"../data/MSC8002/training3d/groundtruths.h5\"]\n",
    "    num_patients = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the class distribution\n",
    "Assigning a higher weight to the positive class (i.e. blood vessels) means the model will pay \"more attention\" to that class. This is useful in the current class imbalance scenario because the number of background (i.e. non-blood vessel) pixels far exceed the number of blood vessel pixels. Without setting a different class weight for the blood vessel class the model would simply assign the background class to all pixels to achieve a low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [settings.CLASS_WEIGHT_BACKGROUND, settings.CLASS_WEIGHT_BLOODVESSEL]\n",
    "print(\"Class distribution: {}\".format(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the 3D U-Net model\n",
    "Instantiate the 3D U-Net model. Use different versions of the `build_model_XXX()` function to try different variations of the model. **Warning**: Changing the model and/or its parameters will change the name of the file the trained model will be saved to. Make sure to update the `VOLVuLuS_test_ipynb` notebook accordingly to ensure it uses the correct saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet_3D_NN(img_height=settings.IMG_HEIGHT,\n",
    "                  img_width=settings.IMG_WIDTH,\n",
    "                  num_slices=settings.SLICE_END - settings.SLICE_START,\n",
    "                  img_channels=settings.IMG_CHANNELS,\n",
    "                  num_classes=settings.NUM_CLASSES)\n",
    "# model = unet.build_model()\n",
    "# model = unet.build_model_3lyr()\n",
    "model = unet.build_model_2lyr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create paths\n",
    "This cell just creates a few paths used later to save training output (e.g. the model architecture, training results and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some path strings\n",
    "model_path = os.path.join(settings.MODEL_PATH, \"VOLVuLuS_\" + unet.title + \"_ep{}.model\".format(settings.TRN_NUM_EPOCH))\n",
    "summ_path = os.path.join(settings.OUTPUT_PATH, \"VOLVuLuS_\" + unet.title + \"_model_summary.txt\")\n",
    "csv_path = os.path.join(settings.OUTPUT_PATH, \"VOLVuLuS_\" + unet.title + \"_training_ep{}_bs{}.csv\".format(settings.TRN_NUM_EPOCH,\n",
    "                                                                                            settings.TRN_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/print model architecture information\n",
    "Save the model's architecture to a file, print it in the cell below and save a diagram to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model_summary_to_file(model, summ_path)\n",
    "model_architecture_to_file(unet.model, settings.OUTPUT_PATH + \"VOLVuLuS_\" + unet.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "Set the loss function, optimiser and metric and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimiser, loss function and metrics\n",
    "opt = Adam()\n",
    "metrics = [dice_coef]\n",
    "loss = weighted_pixelwise_crossentropy_loss(class_weights)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "The cell below loads the training data using data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set generator\n",
    "rdr_train = HDF5Generator_Segment(hdf5_paths[0], hdf5_paths[1],\n",
    "                                  batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                  num_classes=settings.NUM_CLASSES,\n",
    "                                  converter=convert_img_to_pred_3d, \n",
    "                                  feat_key=settings.HDF5_KEY)\n",
    "gen_train = rdr_train.generator(num_epochs=settings.TRN_NUM_EPOCH, dim_reorder=(0, 2, 3, 1, 4))\n",
    "\n",
    "print(\"  Number of training samples: {}\".format(rdr_train.num_images()))\n",
    "print(\"             steps_per_epoch: {}\".format(rdr_train.num_images()/settings.TRN_BATCH_SIZE))\n",
    "\n",
    "\n",
    "if USE_VALIDATION_SET:\n",
    "    # Validation set generator\n",
    "    rdr_val = HDF5Generator_Segment(hdf5_paths[2], hdf5_paths[3],\n",
    "                                    batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                    num_classes=settings.NUM_CLASSES,\n",
    "                                    converter=convert_img_to_pred_3d, \n",
    "                                    feat_key=settings.HDF5_KEY)\n",
    "    gen_val = rdr_val.generator(num_epochs=settings.TRN_NUM_EPOCH, dim_reorder=(0, 2, 3, 1, 4))\n",
    "\n",
    "    print(\"Number of validation samples: {}\".format(rdr_val.num_images()))\n",
    "    print(\"            validation_steps: {}\".format(rdr_val.num_images()/settings.TRN_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Execute the training process, either using generators or using data loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if USE_VALIDATION_SET:\n",
    "    # Prepare callbacks\n",
    "    callbacks = [ModelCheckpoint(model_path, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "                 EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0,\n",
    "                               patience=settings.TRN_EARLY_PATIENCE,\n",
    "                               verbose=0,\n",
    "                               mode=\"auto\"),\n",
    "                 CSVLogger(csv_path, append=False)]\n",
    "\n",
    "    print(\"Training with a validation set.\")\n",
    "    print_training_info(unet, model_path, rdr_train.img_shape, rdr_val.img_shape,\n",
    "                        settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "    # Fit the model using generators and a validation set\n",
    "    hist = model.fit_generator(gen_train,\n",
    "                               epochs=settings.TRN_NUM_EPOCH,\n",
    "                               steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                               verbose=2,\n",
    "                               validation_data=gen_val,\n",
    "                               validation_steps=rdr_val.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                               shuffle=True,\n",
    "                               callbacks=callbacks)\n",
    "else:\n",
    "    callbacks = [ModelCheckpoint(model_path, monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1),\n",
    "                 EarlyStopping(monitor='loss', min_delta=0, patience=settings.TRN_EARLY_PATIENCE, verbose=0, mode=\"auto\"),\n",
    "                 CSVLogger(csv_path, append=False)]\n",
    "\n",
    "    print(\"Training without a validation set.\")\n",
    "    print_training_info(unet, model_path, rdr_train.img_shape,\n",
    "                    None, settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "    # Fit the model using a training set only\n",
    "    start_time = time.time()\n",
    "    hist = model.fit_generator(gen_train,\n",
    "                               epochs=settings.TRN_NUM_EPOCH,\n",
    "                               steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                               verbose=2,\n",
    "                               shuffle=True,\n",
    "                               callbacks=callbacks)\n",
    "\n",
    "print(\"\\n\\nElapsed training time: {:.2f} min.\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot/save the training results\n",
    "Show a plot of the training loss and Dice coefficient by epoch and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist,\n",
    "                      show=True,\n",
    "                      save_path=settings.OUTPUT_PATH + unet.title,\n",
    "                      time_stamp=True,\n",
    "                      metric=\"dice_coef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform pipeline test\n",
    "Use the trained model on one sample in the training data set. This is just to perform pipeline testing during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images and ground truths\n",
    "train_imgs = read_images(hdf5_paths[0], settings.HDF5_KEY, is_3D=True)\n",
    "train_grndtr = read_groundtruths(hdf5_paths[1], settings.HDF5_KEY, is_3D=True)\n",
    "\n",
    "# For pipeline testing only\n",
    "predictions = model.predict(train_imgs, batch_size=settings.TRN_BATCH_SIZE, verbose=2)\n",
    "\n",
    "# Transpose images and ground truths to the correct oder\n",
    "train_imgs = np.transpose(train_imgs, axes=(0, 3, 1, 2, 4))\n",
    "train_grndtr = np.transpose(train_grndtr, axes=(0, 3, 1, 2, 4))\n",
    "\n",
    "# predictions = predictions\n",
    "predictions_imgs = convert_pred_to_img_3d(predictions,\n",
    "                                       threshold=settings.TRN_PRED_THRESHOLD,\n",
    "                                       verbose=settings.VERBOSE)\n",
    "\n",
    "show_image(np.squeeze(train_imgs[0, 0]), 'PRED TRAIN org image')\n",
    "show_image(np.squeeze(train_grndtr[0, 0]), 'PRED TRAIN org ground truth')\n",
    "show_image(np.squeeze(predictions_imgs[0, 0]), 'PRED TRAIN predicted mask')\n",
    "\n",
    "print(\"  original {} dtype {}\".format(np.max(train_imgs[0,0]), train_imgs[0,0].dtype))\n",
    "print(\"  gr truth {} dtype {}\".format(np.max(train_grndtr[0,0]), train_grndtr[0,0].dtype))\n",
    "print(\"prediction {} dtype {}\".format(np.max(predictions_imgs[0,0]), predictions[0,0].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training complete\n",
    "The trained model is now ready to be applied to test MRI images using `VOLuLuS_test.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Cicek]*: Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: learning dense volumetric segmentation from sparse annotation. In *International Conference on Medical Image Computing and Computer- Assisted Intervention*, pages 424–432. Springer, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
