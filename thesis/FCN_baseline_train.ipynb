{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Fully Convolutional Network (FCN) baseline\n",
    "Train an FCN-32s network as defined by [Long (2014)](#References) using limited hyper parameter tuning. Its performance serves as a baseline DECiSION and VOLVuLuS are compared against.\n",
    "\n",
    "The FCN can be trained in two ways: with and without using pre-trained weights obtained after training the model on the ImageNet data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoff/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set Python, NumPy and TensorFlow seeds\n",
    "RANDOM_STATE = 42\n",
    "from numpy.random import seed\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "import random\n",
    "random.seed = RANDOM_STATE\n",
    "\n",
    "# Model and training settings\n",
    "import FCN_baseline_settings as settings\n",
    "\n",
    "# Toolkit imports\n",
    "from dltoolkit.utils.generic import model_architecture_to_file, model_summary_to_file, list_images\n",
    "from dltoolkit.nn.segment import FCN32_NN, FCN32_ImageNet_NN\n",
    "from dltoolkit.utils.visual import plot_training_history, plot_roc_curve,\\\n",
    "    plot_precision_recall_curve, print_confusion_matrix, print_classification_report\n",
    "from dltoolkit.iomisc import HDF5Generator_Segment\n",
    "from dltoolkit.utils.image import to_rgb\n",
    "\n",
    "from thesis_common import convert_img_to_pred, convert_pred_to_img, create_hdf5_db,\\\n",
    "    show_image, read_images, read_groundtruths, print_training_info\n",
    "from thesis_metric_loss import dice_coef_threshold, weighted_pixelwise_crossentropy_loss, evaluate_model\n",
    "\n",
    "# Keras imports\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, TensorBoard,\\\n",
    "    ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import os, cv2, time, progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change how TensorFlow allocates GPU memory\n",
    "Setting `gpu_options.allow_growth` to `True` means TensorFlow will allocate GPU memory as needed rather than using all available memory from the start. This enables monitoring of actual memory usage and determining how close the notebook gets to running out of memory. This has no effect on non-GPU machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a percentage of the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine training settings\n",
    "The constants below determine how the model will be trained:\n",
    "\n",
    "- `USE_VALIDATION_SET`: set to `True` to use a validation set during training, which will be the case most of the time. Set to `False` to not use a validation set, e.g. during pipeline development/validation,\n",
    "- `IS_DEVELOPMENT`: set to `True` to re-create the HDF5 files every time training is executed, `False` otherwise,\n",
    "- `USE_IMAGENET`: set to `True` to use weights pre-trained on ImageNet, `False` otherwise.\n",
    "\n",
    "**Note** if `TRN_TRAIN_VAL_SPLIT` is set to 0, a validation set will *not* be created, even if `USE_VALIDATION_SET` is set to `True`. In fact, `USE_VALIDATION_SET` will be set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_VALIDATION_SET = True\n",
    "IS_DEVELOPMENT = True\n",
    "USE_IMAGENET = True\n",
    "\n",
    "if not USE_VALIDATION_SET:\n",
    "    # If no validation set is to be used override the split value\n",
    "    settings.TRN_TRAIN_VAL_SPLIT = 0.0\n",
    "    \n",
    "if USE_IMAGENET:\n",
    "    NUM_RGB_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert individual JPGs to HDF5 data sets\n",
    "This function converts individual JPG files to HDF5 data sets. It creates up to four HDF5 files depending on whether a portion of the files is set aside as a validation set using the `settings.TRN_TRAIN_VAL_SPLIT` parameter.\n",
    "\n",
    "This conversion only has to be performed once rather than prior to every training run. However, it takes very little time to run. **Note**: if any changes are made to image pre-processing applied to MRA images and/or their ground truths, the conversion MUST be executed prior to the next training run. Failing to do this means training will be performed on existing HDF5 data sets that do not include the revised pre-processing.\n",
    "\n",
    "The folder stucture is shown below. MRA images (i.e. the individial JPG files, one for each *slice*) are located in the `images` folder. Each patient has a separate subfolder. Ground truths are located in the `groundtruths` folder, again each patient has a separate subfolder. The association between MRA image and ground truth is based on the name of the patient's subfolder **and** the alphabetically sorted file name. An MRA image/ground truth pair constitutes one training sample. The resulting HDF5 data sets are stored in the `images` and `ground truths` subfolders. The training data sets are called `train_images.h5` and `train_groundtruths.h5`. The validation sets, if they were created, are called `val_images.h5` and `val_groundtruths.h5`. \n",
    "\n",
    "```\n",
    "---training\n",
    "|     |--- images\n",
    "|     |      |---patient1\n",
    "|     |      |      |--- image1\n",
    "|     |      |      |--- image2\n",
    "|     |      |      ...\n",
    "|     |      |      |--- imageM\n",
    "|     |      |---patient2\n",
    "|     |      ...\n",
    "|     |      |---patientN\n",
    "|     |--- groundtruths\n",
    "|     |      |---patient1\n",
    "|     |      |      |--- groundtruth1\n",
    "|     |      |      |--- groundtruth2\n",
    "|     |      |      ...\n",
    "|     |      |      |--- groundtruthM\n",
    "|     |      |---patient2\n",
    "|     |      ...\n",
    "|     |      |---patientN\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdf5_conversion(settings):\n",
    "    \"\"\"Convert the individual .jpg files (images and ground truths) to four .hdf5 files:\n",
    "    - Two with the training set (train_images.h5 and train_groundtruths.h5)\n",
    "    - Two with the validation set (val_images.h5 and val_groundtruths.h5)\n",
    "    \n",
    "    If TRN_TRAIN_VAL_SPLIT is set to zero NO validation data sets are generated.\n",
    "    \n",
    "    Returns the path to the HDF5 files and the number of patient folders processed.\n",
    "    \"\"\"\n",
    "    # Prepare the path to the training images and ground truths\n",
    "    img_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_IMAGES)\n",
    "    msk_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_GROUND_TRUTH)\n",
    "\n",
    "    # Create a list of paths to the individual patient folders\n",
    "    patient_fld_imgs = sorted([os.path.join(img_path, e.name)\n",
    "                               for e in os.scandir(img_path) if e.is_dir()])\n",
    "    patient_fld_masks = sorted([os.path.join(msk_path, e.name)\n",
    "                                for e in os.scandir(msk_path) if e.is_dir()])\n",
    "\n",
    "    # Obtain a list of paths to the training images and ground truths for each patient\n",
    "    img_list = []\n",
    "    msk_list = []\n",
    "    for patient_ix, (p_fld_imgs, p_fld_masks) in enumerate(zip(patient_fld_imgs,\n",
    "                                                               patient_fld_masks)):\n",
    "        img_list.extend(sorted(list(list_images(basePath=p_fld_imgs,\n",
    "                                                validExts=settings.IMG_EXTENSION)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "        msk_list.extend(sorted(list(list_images(basePath=p_fld_masks,\n",
    "                                                validExts=settings.IMG_EXTENSION)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "\n",
    "    assert(len(img_list) == len(msk_list))\n",
    "\n",
    "    # Split the training set into a training and validation set. Consecutive slices no longer\n",
    "    # belong to the same patient\n",
    "    train_img, val_img, train_msk, val_msk = train_test_split(img_list, msk_list,\n",
    "                                                              test_size=settings.TRN_TRAIN_VAL_SPLIT,\n",
    "                                                              random_state=settings.RANDOM_STATE,\n",
    "                                                              shuffle=True)\n",
    "    \n",
    "    # Create the HDF5 data sets\n",
    "    output_paths = []\n",
    "\n",
    "    # Training images\n",
    "    output_paths.append(create_hdf5_db(train_img, \"train\", img_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                       settings=settings))\n",
    "\n",
    "    # Training ground truths\n",
    "    output_paths.append(create_hdf5_db(train_msk, \"train\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                       settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    # Validation images\n",
    "    output_paths.append(create_hdf5_db(val_img, \"val\", img_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                       settings=settings))\n",
    "\n",
    "    # Validation ground truths\n",
    "    output_paths.append(create_hdf5_db(val_msk, \"val\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT,\n",
    "                                       settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    return output_paths, len(patient_fld_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converted images to HDF5, full paths:\n",
      "\n",
      "../data/MSC8002/training/train_images.h5\n",
      "../data/MSC8002/training/train_groundtruths.h5\n",
      "../data/MSC8002/training/val_images.h5\n",
      "../data/MSC8002/training/val_groundtruths.h5\n",
      "\n",
      "Created a 0.10 split.\n"
     ]
    }
   ],
   "source": [
    "if IS_DEVELOPMENT:\n",
    "    print(\"\\n--- Converted images to HDF5, full paths:\\n\")\n",
    "        \n",
    "    hdf5_paths, num_patients = perform_hdf5_conversion(settings)\n",
    "    \n",
    "    # If TRN_TRAIN_VAL_SPLIT is set to 0 don't use a validation set, but warn the user\n",
    "    if settings.TRN_TRAIN_VAL_SPLIT == 0:\n",
    "        print(\"\\nValidation split is 0.0, validation set NOT created.\")\n",
    "        USE_VALIDATION_SET = False\n",
    "    else:\n",
    "        print(\"\\nCreated a {:.2f} split.\".format(settings.TRN_TRAIN_VAL_SPLIT))\n",
    "else:\n",
    "    # Use existing HDF5 data sets\n",
    "    hdf5_paths = [\"../data/MSC8002/training/train_images.h5\",\n",
    "                  \"../data/MSC8002/training/train_groundtruths.h5\",\n",
    "                  \"../data/MSC8002/training/val_images.h5\",\n",
    "                  \"../data/MSC8002/training/val_groundtruths.h5\"]\n",
    "    num_patients = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the class distribution\n",
    "Assigning a higher weight to the positive class (i.e. blood vessels) means the model will pay \"more attention\" to that class. This is useful in the current class imbalance scenario because the number of background (i.e. non-blood vessel) pixels far exceed the number of blood vessel pixels. Without setting a different class weight for the blood vessel class the model would simply assign the background class to all pixels to achieve a low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [1.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "class_weights = [settings.CLASS_WEIGHT_BACKGROUND, settings.CLASS_WEIGHT_BLOODVESSEL]\n",
    "print(\"Class distribution: {}\".format(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the FCN model\n",
    "Instantiate the FCN-32s model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if USE_IMAGENET:\n",
    "    fcn = FCN32_ImageNet_NN(settings.IMG_HEIGHT, settings.IMG_WIDTH, NUM_RGB_CHANNELS, settings.NUM_CLASSES)\n",
    "else:\n",
    "    fcn = FCN32_NN(settings.IMG_HEIGHT, settings.IMG_WIDTH, settings.IMG_CHANNELS, settings.NUM_CLASSES)\n",
    "\n",
    "# Crop 16 pixels on all sides to get 256x256 images\n",
    "model = fcn.build_model(crop=16, use_bn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create paths\n",
    "This cell just creates a few paths used later to save training output (e.g. the model architecture, training results and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prefix = \"FCN_baseline_\" + fcn.title + \"_W\"+ str(settings.CLASS_WEIGHT_BLOODVESSEL) + \"_\" + settings.TRN_LOSS + \"_BS\" + \"{:03}\".format(settings.TRN_BATCH_SIZE)\n",
    "\n",
    "model_path = os.path.join(settings.MODEL_PATH, prefix + \".model\")\n",
    "summ_path = os.path.join(settings.OUTPUT_PATH, prefix + \"_model_summary.txt\")\n",
    "csv_path = os.path.join(settings.OUTPUT_PATH, prefix + \"_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/print model architecture information\n",
    "Save the model's architecture to a file, print it in the cell below and save a diagram to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 4096)        16384     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 4096)        16384     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 6)           24582     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 288, 288, 6)       147456    \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 256, 256, 3, 2)    0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 256, 3, 2)    0         \n",
      "=================================================================\n",
      "Total params: 134,465,350\n",
      "Trainable params: 134,448,966\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model_summary_to_file(model, summ_path)\n",
    "model_architecture_to_file(fcn.model, os.path.join(settings.OUTPUT_PATH, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "Set the loss function, optimiser and metric and compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compile_model(model, lr):\n",
    "    # Set the optimiser, loss function and metrics\n",
    "    if settings.TRN_LOSS == \"ADAM\":\n",
    "        opt = Adam(lr=lr, amsgrad=settings.TRN_AMS_GRAD)\n",
    "    else:\n",
    "        opt = SGD(lr=lr)\n",
    "\n",
    "    metrics = [dice_coef_threshold(settings.TRN_PRED_THRESHOLD)]\n",
    "    loss = weighted_pixelwise_crossentropy_loss(class_weights)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model, opt, loss\n",
    "    \n",
    "model, opt, loss = compile_model(model, settings.TRN_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Load data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image HDF5: ../data/MSC8002/training/train_images.h5 with dtype = float32\n",
      "Loading ground truth HDF5: ../data/MSC8002/training/train_groundtruths.h5 with dtype = uint8\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "Elapsed time: 0.01s\n",
      "  Number of training samples: 1, gr.tr.(1, 256, 256, 3, 2)\n",
      "\n",
      "Loading image HDF5: ../data/MSC8002/training/val_images.h5 with dtype = float32\n",
      "Loading ground truth HDF5: ../data/MSC8002/training/val_groundtruths.h5 with dtype = uint8\n",
      "(1, 256, 256, 1)\n",
      "(1, 256, 256, 1)\n",
      "Elapsed time: 0.00s\n",
      "Number of validation samples: 1, gr.tr.(1, 256, 256, 3, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_imgs = read_images(hdf5_paths[0], settings.HDF5_KEY)\n",
    "train_grndtr = read_groundtruths(hdf5_paths[1], settings.HDF5_KEY)\n",
    "\n",
    "if USE_IMAGENET:\n",
    "    train_imgs = to_rgb(train_imgs)\n",
    "    train_grndtr = to_rgb(train_grndtr)\n",
    "    \n",
    "train_grndtr_ext_conv = convert_img_to_pred(train_grndtr,\n",
    "                                        settings.NUM_CLASSES,\n",
    "                                        settings.VERBOSE)\n",
    "\n",
    "print(\"  Number of training samples: {}, gr.tr.{}\\n\".format(len(train_imgs), train_grndtr_ext_conv.shape))\n",
    "\n",
    "# Read the validation set\n",
    "if USE_VALIDATION_SET:\n",
    "    val_imgs = read_images(hdf5_paths[2], settings.HDF5_KEY)\n",
    "    val_grndtr = read_groundtruths(hdf5_paths[3], settings.HDF5_KEY)\n",
    "    if USE_IMAGENET:\n",
    "        val_imgs = to_rgb(val_imgs)\n",
    "        val_grndtr = to_rgb(val_grndtr)\n",
    "    \n",
    "    val_grndtr_ext_conv = convert_img_to_pred(val_grndtr,\n",
    "                                          settings.NUM_CLASSES,\n",
    "                                          settings.VERBOSE)\n",
    "    print(\"Number of validation samples: {}, gr.tr.{}\\n\".format(len(val_imgs), val_grndtr_ext_conv.shape))\n",
    "else:\n",
    "    print(\"Not loading a validation set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare callbacks\n",
    "Prepare callbacks used during training:\n",
    "\n",
    "- TensorBoard: basic TensorBoard visualizations (not always used)\n",
    "- EarlyStopping: Stop training when a monitored quantity has stopped improving\n",
    "- CSVLogger: streams epoch results to a csv file\n",
    "- ModelCheckpoint: save the model after every epoch\n",
    "- ReduceLROPlateau: reduce the learning when progress halts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks():\n",
    "    if USE_VALIDATION_SET:\n",
    "        loss_str = \"val_loss\"\n",
    "    else:\n",
    "        loss_str = \"loss\"\n",
    "\n",
    "    cvs_callb = CSVLogger(csv_path, append=False)\n",
    "\n",
    "\n",
    "    red_callb = ReduceLROnPlateau(monitor=loss_str,\n",
    "                              factor=settings.TRN_PLAT_FACTOR,\n",
    "                              patience=settings.TRN_PLAT_PATIENCE,\n",
    "                              verbose=1,\n",
    "                              mode=\"min\")\n",
    "\n",
    "    mc_callb = ModelCheckpoint(model_path,\n",
    "                               monitor=loss_str,\n",
    "                               mode=\"min\",\n",
    "                               save_best_only=True,\n",
    "                               save_weights_only=True,\n",
    "                               verbose=1)\n",
    "\n",
    "    es_callb = EarlyStopping(monitor=loss_str,\n",
    "                             min_delta=0,\n",
    "                             patience=settings.TRN_EARLY_PATIENCE,\n",
    "                             verbose=0,\n",
    "                             mode=\"auto\")\n",
    "\n",
    "    return [mc_callb, es_callb, cvs_callb, red_callb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Execute the training process using data loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_training(num_epoch):\n",
    "    callbacks = create_callbacks()\n",
    "\n",
    "    if USE_VALIDATION_SET:\n",
    "        print(\"Training with a validation set, using all data in memory.\")\n",
    "        print_training_info(fcn, model_path, train_imgs.shape, val_imgs.shape,\n",
    "                            settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "        # Fit the model using a training and validation set\n",
    "        hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                         epochs=num_epoch,\n",
    "                         batch_size=settings.TRN_BATCH_SIZE,\n",
    "                         verbose=2,\n",
    "                         shuffle=True,\n",
    "                         validation_data=(val_imgs, val_grndtr_ext_conv),\n",
    "                         callbacks=callbacks)\n",
    "    else:\n",
    "        print(\"Training without a validation set, using all data in memory.\")\n",
    "        print_training_info(fcn, model_path, train_imgs.shape, None,\n",
    "                            settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "        # Fit the model using a training set only\n",
    "        hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                         epochs=num_epoch,\n",
    "                         batch_size=settings.TRN_BATCH_SIZE,\n",
    "                         verbose=2,\n",
    "                         shuffle=True,\n",
    "                         callbacks=callbacks)\n",
    "        \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: model \"warm-up\"\n",
      "=================================\n",
      "Training with a validation set, using all data in memory.\n",
      "\n",
      "Generic information:\n",
      "              Model: FCN32-ImageNet_BN\n",
      "          Saving to: ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      " Number of patients: 1\n",
      "     Training shape: (1, 256, 256, 3) (1 per patient)\n",
      "   Validation shape: (1, 256, 256, 3) (1 per patient)\n",
      "      Class weights: [1.0, 10.0]\n",
      "\n",
      "Hyper parameters:\n",
      "          Optimizer: <class 'keras.optimizers.Adam'>\n",
      "                   : 0 = ('lr', 0.009999999776482582)\n",
      "                   : 1 = ('beta_1', 0.8999999761581421)\n",
      "                   : 2 = ('beta_2', 0.9990000128746033)\n",
      "                   : 3 = ('decay', 0.0)\n",
      "                   : 4 = ('epsilon', 1e-07)\n",
      "                   : 5 = ('amsgrad', True)\n",
      "       TRN_AMS_GRAD: True\n",
      "               Loss: <function weighted_pixelwise_crossentropy_loss\n",
      "         IMG_HEIGHT: 256\n",
      "          IMG_WIDTH: 256\n",
      "       IMG_CHANNELS: 1\n",
      "        NUM_CLASSES: 2\n",
      "        SLICE_START: 58\n",
      "          SLICE_END: 60\n",
      "    IMG_CROP_HEIGHT: 32\n",
      "     IMG_CROP_WIDTH: 32\n",
      "\n",
      "     TRN_BATCH_SIZE: 1\n",
      "  TRN_LEARNING_RATE: 0.001\n",
      "      TRN_NUM_EPOCH: 500\n",
      "TRN_TRAIN_VAL_SPLIT: 0.1\n",
      "   TRN_DROPOUT_RATE: 0.5\n",
      "       TRN_MOMENTUM: 0.99\n",
      " TRN_PRED_THRESHOLD: 0.5\n",
      " TRN_EARLY_PATIENCE: 9\n",
      "    TRN_PLAT_FACTOR: 0.2\n",
      "  TRN_PLAT_PATIENCE: 3\n",
      "\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/10\n",
      " - 28s - loss: 141288.4219 - dice_coef_t: 0.5001 - val_loss: 1042967.0625 - val_dice_coef_t: 0.6909\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1042967.06250, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 2/10\n",
      " - 6s - loss: 207123.5000 - dice_coef_t: 0.4706 - val_loss: 1881709.7500 - val_dice_coef_t: 0.4290\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1042967.06250\n",
      "Epoch 3/10\n",
      " - 7s - loss: 198490.2031 - dice_coef_t: 0.5325 - val_loss: 1827111.2500 - val_dice_coef_t: 0.4397\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1042967.06250\n",
      "Epoch 4/10\n",
      " - 8s - loss: 157168.5938 - dice_coef_t: 0.4746 - val_loss: 1718121.3750 - val_dice_coef_t: 0.4206\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1042967.06250\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 5/10\n",
      " - 8s - loss: 188979.4531 - dice_coef_t: 0.5243 - val_loss: 563084.1250 - val_dice_coef_t: 0.4274\n",
      "\n",
      "Epoch 00005: val_loss improved from 1042967.06250 to 563084.12500, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 6/10\n",
      " - 7s - loss: 160893.0312 - dice_coef_t: 0.5617 - val_loss: 282827.6250 - val_dice_coef_t: 0.5086\n",
      "\n",
      "Epoch 00006: val_loss improved from 563084.12500 to 282827.62500, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 7/10\n",
      " - 6s - loss: 137207.2344 - dice_coef_t: 0.6346 - val_loss: 222952.1719 - val_dice_coef_t: 0.5646\n",
      "\n",
      "Epoch 00007: val_loss improved from 282827.62500 to 222952.17188, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 8/10\n",
      " - 6s - loss: 133128.6719 - dice_coef_t: 0.6596 - val_loss: 196135.0625 - val_dice_coef_t: 0.6035\n",
      "\n",
      "Epoch 00008: val_loss improved from 222952.17188 to 196135.06250, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 9/10\n",
      " - 6s - loss: 132109.9688 - dice_coef_t: 0.6634 - val_loss: 242054.3594 - val_dice_coef_t: 0.6293\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 196135.06250\n",
      "Epoch 10/10\n",
      " - 6s - loss: 126910.7969 - dice_coef_t: 0.6780 - val_loss: 120373.9922 - val_dice_coef_t: 0.7143\n",
      "\n",
      "Epoch 00010: val_loss improved from 196135.06250 to 120373.99219, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "\n",
      "\n",
      "Elapsed warm-up time: 1.73 min.\n",
      "\n",
      "Stage 2: train the entire network\n",
      "=================================\n",
      "Training with a validation set, using all data in memory.\n",
      "\n",
      "Generic information:\n",
      "              Model: FCN32-ImageNet_BN\n",
      "          Saving to: ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      " Number of patients: 1\n",
      "     Training shape: (1, 256, 256, 3) (1 per patient)\n",
      "   Validation shape: (1, 256, 256, 3) (1 per patient)\n",
      "      Class weights: [1.0, 10.0]\n",
      "\n",
      "Hyper parameters:\n",
      "          Optimizer: <class 'keras.optimizers.Adam'>\n",
      "                   : 0 = ('lr', 9.999999747378752e-05)\n",
      "                   : 1 = ('beta_1', 0.8999999761581421)\n",
      "                   : 2 = ('beta_2', 0.9990000128746033)\n",
      "                   : 3 = ('decay', 0.0)\n",
      "                   : 4 = ('epsilon', 1e-07)\n",
      "                   : 5 = ('amsgrad', True)\n",
      "       TRN_AMS_GRAD: True\n",
      "               Loss: <function weighted_pixelwise_crossentropy_loss\n",
      "         IMG_HEIGHT: 256\n",
      "          IMG_WIDTH: 256\n",
      "       IMG_CHANNELS: 1\n",
      "        NUM_CLASSES: 2\n",
      "        SLICE_START: 58\n",
      "          SLICE_END: 60\n",
      "    IMG_CROP_HEIGHT: 32\n",
      "     IMG_CROP_WIDTH: 32\n",
      "\n",
      "     TRN_BATCH_SIZE: 1\n",
      "  TRN_LEARNING_RATE: 0.0001\n",
      "      TRN_NUM_EPOCH: 500\n",
      "TRN_TRAIN_VAL_SPLIT: 0.1\n",
      "   TRN_DROPOUT_RATE: 0.5\n",
      "       TRN_MOMENTUM: 0.99\n",
      " TRN_PRED_THRESHOLD: 0.5\n",
      " TRN_EARLY_PATIENCE: 9\n",
      "    TRN_PLAT_FACTOR: 0.2\n",
      "  TRN_PLAT_PATIENCE: 3\n",
      "\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/500\n",
      " - 25s - loss: 120272.1797 - dice_coef_t: 0.7000 - val_loss: 117191.5234 - val_dice_coef_t: 0.7239\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 117191.52344, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 2/500\n",
      " - 7s - loss: 118101.6953 - dice_coef_t: 0.7115 - val_loss: 115762.5391 - val_dice_coef_t: 0.7344\n",
      "\n",
      "Epoch 00002: val_loss improved from 117191.52344 to 115762.53906, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 3/500\n",
      " - 6s - loss: 116399.0156 - dice_coef_t: 0.7217 - val_loss: 114173.1953 - val_dice_coef_t: 0.7454\n",
      "\n",
      "Epoch 00003: val_loss improved from 115762.53906 to 114173.19531, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 4/500\n",
      " - 6s - loss: 114593.5234 - dice_coef_t: 0.7336 - val_loss: 112701.2500 - val_dice_coef_t: 0.7561\n",
      "\n",
      "Epoch 00004: val_loss improved from 114173.19531 to 112701.25000, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 5/500\n",
      " - 6s - loss: 112962.1641 - dice_coef_t: 0.7445 - val_loss: 111323.7266 - val_dice_coef_t: 0.7672\n",
      "\n",
      "Epoch 00005: val_loss improved from 112701.25000 to 111323.72656, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 6/500\n",
      " - 8s - loss: 111481.6875 - dice_coef_t: 0.7552 - val_loss: 110019.1797 - val_dice_coef_t: 0.7763\n",
      "\n",
      "Epoch 00006: val_loss improved from 111323.72656 to 110019.17969, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 7/500\n",
      " - 8s - loss: 110133.5547 - dice_coef_t: 0.7641 - val_loss: 108769.5938 - val_dice_coef_t: 0.7848\n",
      "\n",
      "Epoch 00007: val_loss improved from 110019.17969 to 108769.59375, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 8/500\n",
      " - 8s - loss: 108890.4375 - dice_coef_t: 0.7725 - val_loss: 107578.4609 - val_dice_coef_t: 0.7918\n",
      "\n",
      "Epoch 00008: val_loss improved from 108769.59375 to 107578.46094, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 9/500\n",
      " - 8s - loss: 107744.1719 - dice_coef_t: 0.7798 - val_loss: 106390.3594 - val_dice_coef_t: 0.7980\n",
      "\n",
      "Epoch 00009: val_loss improved from 107578.46094 to 106390.35938, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 10/500\n",
      " - 8s - loss: 106639.7188 - dice_coef_t: 0.7858 - val_loss: 105216.7188 - val_dice_coef_t: 0.8038\n",
      "\n",
      "Epoch 00010: val_loss improved from 106390.35938 to 105216.71875, saving model to ../savedmodels/FCN_baseline_FCN32-ImageNet_BN_W10.0_ADAM_BS001.model\n",
      "Epoch 11/500\n",
      " - 8s - loss: 105573.5781 - dice_coef_t: 0.7912 - val_loss: 119095.3750 - val_dice_coef_t: 0.8079\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 105216.71875\n",
      "Epoch 12/500\n",
      " - 7s - loss: 104530.8125 - dice_coef_t: 0.7963 - val_loss: 123210.9219 - val_dice_coef_t: 0.8127\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 105216.71875\n",
      "Epoch 13/500\n",
      " - 7s - loss: 103498.2344 - dice_coef_t: 0.8010 - val_loss: 123660.1641 - val_dice_coef_t: 0.8176\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 105216.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 14/500\n",
      " - 7s - loss: 102466.8672 - dice_coef_t: 0.8052 - val_loss: 123811.1562 - val_dice_coef_t: 0.8180\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 105216.71875\n",
      "Epoch 15/500\n"
     ]
    }
   ],
   "source": [
    "if USE_IMAGENET:\n",
    "    # First warm up the model by freezing the VGG layers\n",
    "    start_time = time.time()\n",
    "    print(\"Stage 1: model \\\"warm-up\\\"\")\n",
    "    print(\"=================================\")\n",
    "    fcn.freeze_vgg_layers(True)    \n",
    "    model, opt, loss = compile_model(model, settings.TRN_LEARNING_RATE_WARMUP)\n",
    "    perform_training(settings.TRN_NUM_EPOCH_WARMUP)\n",
    "    print(\"\\n\\nElapsed warm-up time: {:.2f} min.\".format(int((time.time() - start_time))/60))\n",
    "    \n",
    "    # Then unfreeze all layers and train the entire model using a very small learning rate\n",
    "    fcn.freeze_vgg_layers(False)\n",
    "    settings.TRN_LEARNING_RATE = 0.0001\n",
    "    model, opt, loss = compile_model(model, settings.TRN_LEARNING_RATE)\n",
    "    print(\"\\nStage 2: train the entire network\")\n",
    "    print(\"=================================\")\n",
    "\n",
    "start_time = time.time()\n",
    "hist = perform_training(settings.TRN_NUM_EPOCH)\n",
    "print(\"\\n\\nElapsed training time: {:.2f} min.\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot/save the training results\n",
    "Show a plot of the training loss and Dice coefficient by epoch and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_training_history(hist,\n",
    "                      show=True,\n",
    "                      save_path=os.path.join(settings.OUTPUT_PATH, prefix),\n",
    "                      time_stamp=True,\n",
    "                      metric=\"dice_coef_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform pipeline test\n",
    "Use the trained model on one sample in the training data set. This is just to perform pipeline testing during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read training images and ground truths\n",
    "train_imgs = read_images(hdf5_paths[0], settings.HDF5_KEY)\n",
    "train_grndtr = read_groundtruths(hdf5_paths[1], settings.HDF5_KEY)\n",
    "\n",
    "if USE_IMAGENET:\n",
    "    train_imgs = to_rgb(train_imgs)\n",
    "    train_grndtr = to_rgb(train_grndtr)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(train_imgs, batch_size=settings.TRN_BATCH_SIZE, verbose=2)\n",
    "print(\"Predictions shape: {}\".format(predictions.shape))\n",
    "\n",
    "# Convert predictions to images\n",
    "if USE_IMAGENET:\n",
    "    predictions_imgs = convert_pred_to_img(predictions,\n",
    "                                       threshold=settings.TRN_PRED_THRESHOLD,\n",
    "                                       num_channels=NUM_RGB_CHANNELS,\n",
    "                                       verbose=settings.VERBOSE)\n",
    "\n",
    "else:\n",
    "    predictions_imgs = convert_pred_to_img(predictions,\n",
    "                                       threshold=settings.TRN_PRED_THRESHOLD,\n",
    "                                       num_channels=settings.IMG_CHANNELS,\n",
    "                                       verbose=settings.VERBOSE)\n",
    "\n",
    "# Show a single image, ground truth and segmentation map\n",
    "show_image(np.squeeze(predictions_imgs[0]), 'Segmentation map')\n",
    "show_image(np.squeeze(train_grndtr[0]), 'Ground truth')\n",
    "show_image(np.squeeze(train_imgs[0]), 'Original image')\n",
    "\n",
    "print(\"segmentation map {} dtype {}\".format(np.max(predictions_imgs[0]),\n",
    "                                            predictions_imgs[0].dtype))\n",
    "print(\"    ground truth {} dtype {}\".format(np.max(train_grndtr[0]),\n",
    "                                            train_grndtr[0].dtype))\n",
    "print(\"        original {} dtype {}\".format(np.max(train_imgs[0]),\n",
    "                                            train_imgs[0].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Training complete\n",
    "The trained model is now ready to be applied to test MRA images using `FCN_baseline_test.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Long]*: Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation. In *Corr*, abs/1411.4038, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
