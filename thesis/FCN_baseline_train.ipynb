{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Fully Convolutional Network (FCN) baseline\n",
    "Train an FCN-32s network as defined by [Shelhamer (2017)](#References) using limited hyper parameter tuning. Its performance serves as a baseline DECiSION and VOLVuLuS are compared against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seeds and import packages\n",
    "Setting the seeds first is meant to achieve reproducibility, though this goal is not entirely achieved. **Note**: too keep things simple, the FCN and DECIsION models use the same settings (both load `DECiSION_settings`.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoff/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set Python, NumPy and TensorFlow seeds\n",
    "RANDOM_STATE = 42\n",
    "from numpy.random import seed\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "import random\n",
    "random.seed = RANDOM_STATE\n",
    "\n",
    "# Model and training settings\n",
    "import DECiSION_settings as settings\n",
    "\n",
    "# Toolkit imports\n",
    "from dltoolkit.utils.generic import model_architecture_to_file, model_summary_to_file, list_images\n",
    "from dltoolkit.nn.segment import FCN32_NN\n",
    "from dltoolkit.utils.visual import plot_training_history, plot_roc_curve,\\\n",
    "    plot_precision_recall_curve, print_confusion_matrix, print_classification_report\n",
    "from dltoolkit.iomisc import HDF5Generator_Segment\n",
    "\n",
    "from thesis_common import convert_img_to_pred, convert_pred_to_img, create_hdf5_db,\\\n",
    "    show_image, read_images, read_groundtruths, print_training_info\n",
    "from thesis_metric_loss import dice_coef, weighted_pixelwise_crossentropy_loss\n",
    "\n",
    "# Keras imports\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import os, cv2, time, progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change how TensorFlow allocates GPU memory\n",
    "Setting `gpu_options.allow_growth` to `True` means TensorFlow will allocate GPU memory as needed rather than using all available memory from the start. This enables monitoring of actual memory usage and determining how close the notebook gets to running out of memory. This has no effect on non-GPU machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a percentage of the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine training settings\n",
    "The variables below determine how the model will be trained:\n",
    "\n",
    "- `USE_GENERATORS`: when set to `True` the training process will use data generators, which is typically what will be required given the size of the models being trained and the size of the images. Set to `False` to (try to) load all training data into memory.\n",
    "- `USE_VALIDATION_SET`: set to `True` to use a validation set during training, which will be the case most of the time. set to `False` to not use a validation set, e.g. during pipeline development/validation.\n",
    "- `IS_DEVELOPMENT`: set to `True` to re-create the HDF5 files every time training is executed, `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GENERATORS = True\n",
    "USE_VALIDATION_SET = False\n",
    "IS_DEVELOPMENT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert individual JPGs to HDF5 data sets\n",
    "This function converts individual JPG files to HDF5 data sets. It creates up to four HDF5 files depending on whether a portion of the files is set aside as a validation set using the `settings.TRN_TRAIN_VAL_SPLIT` parameter.\n",
    "\n",
    "This conversion only has to be performed once rather than prior to every training run. However, it takes very little time to run. **Note**: if any changes are made to image pre-processing applied to MRI images and/or their ground truths, the conversion MUST be executed prior to the next training run. Failing to do this means training will be performed on existing HDF5 data sets that do not include the revised pre-processing.\n",
    "\n",
    "The folder stucture is shown below. MRI images (i.e. the individial JPG files, one for each *slice*) are located in the `images` folder. Each patient has a separate subfolder. Ground truths are located in the `groundtruths` folder, again each patient has a separate subfolder. The association between MRI image and ground truth is based on the name of the patient's subfolder **and** the alphabetically sorted file name. An MRI image/ground truth pair constitutes one training sample. The resulting HDF5 data sets are stored in the `images` and `ground truths` subfolders. The training data sets are called `train_images.h5` and `train_groundtruths.h5`. The validation sets, if they were created, are called `val_images.h5` and `val_groundtruths.h5`. \n",
    "\n",
    "```\n",
    "---training\n",
    "|     |--- images\n",
    "|     |      |---patient1\n",
    "|     |      |      |--- image1\n",
    "|     |      |      |--- image2\n",
    "|     |      |      ...\n",
    "|     |      |      |--- imageM\n",
    "|     |      |---patient2\n",
    "|     |      ...\n",
    "|     |      |---patientN\n",
    "|     |--- groundtruths\n",
    "|     |      |---patient1\n",
    "|     |      |      |--- groundtruth1\n",
    "|     |      |      |--- groundtruth2\n",
    "|     |      |      ...\n",
    "|     |      |      |--- groundtruthM\n",
    "|     |      |---patient2\n",
    "|     |      ...\n",
    "|     |      |---patientN\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def perform_hdf5_conversion(settings):\n",
    "    \"\"\"Convert the individual .jpg files (images and ground truths) to four .hdf5 files:\n",
    "    - Two with the training set (train_images.h5 and train_groundtruths.h5)\n",
    "    - Two with the validation set (val_images.h5 and val_groundtruths.h5)\n",
    "    \n",
    "    If TRN_TRAIN_VAL_SPLIT is set to zero NO validation data sets are generated.\n",
    "    \n",
    "    Returns the path to the HDF5 files and the number of patient folders processed.\n",
    "    \"\"\"\n",
    "    # Prepare the path to the training images and ground truths\n",
    "    img_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_IMAGES)\n",
    "    msk_path = os.path.join(settings.TRAINING_PATH, settings.FLDR_GROUND_TRUTH)\n",
    "\n",
    "    # Create a list of paths to the individual patient folders\n",
    "    patient_fld_imgs = sorted([os.path.join(img_path, e.name)\n",
    "                               for e in os.scandir(img_path) if e.is_dir()])\n",
    "    patient_fld_masks = sorted([os.path.join(msk_path, e.name)\n",
    "                                for e in os.scandir(msk_path) if e.is_dir()])\n",
    "\n",
    "    # Obtain a list of paths to the training images and ground truths for each patient\n",
    "    img_list = []\n",
    "    msk_list = []\n",
    "    for patient_ix, (p_fld_imgs, p_fld_masks) in enumerate(zip(patient_fld_imgs,\n",
    "                                                               patient_fld_masks)):\n",
    "        img_list.extend(sorted(list(list_images(basePath=p_fld_imgs,\n",
    "                                                validExts=settings.IMG_EXTENSION)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "        msk_list.extend(sorted(list(list_images(basePath=p_fld_masks,\n",
    "                                                validExts=settings.IMG_EXTENSION)))\n",
    "                        [settings.SLICE_START:settings.SLICE_END])\n",
    "\n",
    "    assert(len(img_list) == len(msk_list))\n",
    "\n",
    "    # Split the training set into a training and validation set. Consecutive slices no longer\n",
    "    # belong to the same patient\n",
    "    train_img, val_img, train_msk, val_msk = train_test_split(img_list, msk_list,\n",
    "                                                              test_size=settings.TRN_TRAIN_VAL_SPLIT,\n",
    "                                                              random_state=settings.RANDOM_STATE,\n",
    "                                                              shuffle=True)\n",
    "    \n",
    "    # Create the HDF5 data sets\n",
    "    output_paths = []\n",
    "\n",
    "    # Training images\n",
    "    output_paths.append(create_hdf5_db(train_img, \"train\", img_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings))\n",
    "\n",
    "    # Training ground truths\n",
    "    output_paths.append(create_hdf5_db(train_msk, \"train\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    # Validation images\n",
    "    output_paths.append(create_hdf5_db(val_img, \"val\", img_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings))\n",
    "\n",
    "    # Validation ground truths\n",
    "    output_paths.append(create_hdf5_db(val_msk, \"val\", msk_path,\n",
    "                                       (settings.IMG_HEIGHT,\n",
    "                                        settings.IMG_WIDTH,\n",
    "                                        settings.IMG_CHANNELS),\n",
    "                                       key=settings.HDF5_KEY, ext=settings.HDF5_EXT, settings=settings,\n",
    "                                       is_mask=True))\n",
    "\n",
    "    return output_paths, len(patient_fld_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n",
      "Creating HDF5 database 100% |###################################| Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converted images to HDF5, full paths:\n",
      "\n",
      "../data/MSC8002/training/train_images.h5\n",
      "../data/MSC8002/training/train_groundtruths.h5\n",
      "../data/MSC8002/training/val_images.h5\n",
      "../data/MSC8002/training/val_groundtruths.h5\n",
      "\n",
      "Created a 0.10 split.\n"
     ]
    }
   ],
   "source": [
    "if IS_DEVELOPMENT:\n",
    "    print(\"\\n--- Converted images to HDF5, full paths:\\n\")\n",
    "    hdf5_paths, num_patients = perform_hdf5_conversion(settings)\n",
    "    \n",
    "    if settings.TRN_TRAIN_VAL_SPLIT == 0:\n",
    "        print(\"\\nValidation set NOT created.\")\n",
    "        USE_VALIDATION_SET = False\n",
    "    else:\n",
    "        print(\"\\nCreated a {:.2f} split.\".format(settings.TRN_TRAIN_VAL_SPLIT))\n",
    "else:\n",
    "    # Use existing HDF5 data sets\n",
    "    hdf5_paths = [\"../data/MSC8002/training/train_images.h5\",\n",
    "                  \"../data/MSC8002/training/train_groundtruths.h5\",\n",
    "                  \"../data/MSC8002/training/val_images.h5\",\n",
    "                  \"../data/MSC8002/training/val_groundtruths.h5\"]\n",
    "    num_patients = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the class distribution\n",
    "Assigning a higher weight to the positive class (i.e. blood vessels) means the model will pay \"more attention\" to that class. This is useful in the current class imbalance scenario because the number of background (i.e. non-blood vessel) pixels far exceed the number of blood vessel pixels. Without setting a different class weight for the blood vessel class the model would simply assign the background class to all pixels to achieve a low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [1.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "class_weights = [settings.CLASS_WEIGHT_BACKGROUND, settings.CLASS_WEIGHT_BLOODVESSEL]\n",
    "print(\"Class distribution: {}\".format(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the FCN model\n",
    "Instantiate the FCN-32s model. **Warning**: Changing the model and/or its parameters will change the name of the file the trained model will be saved to. Make sure to update the `FCN_baseline_test_ipynb` notebook accordingly to ensure it uses the correct saved model. **Note**: contrary to the paper, the model does not load weights pre-trained on, for example, ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fcn = FCN32_NN(img_height=settings.IMG_HEIGHT,\n",
    "               img_width=settings.IMG_WIDTH,\n",
    "               img_channels=settings.IMG_CHANNELS,\n",
    "               num_classes=settings.NUM_CLASSES,\n",
    "               dropout_rate=settings.TRN_DROPOUT_RATE)\n",
    "\n",
    "# Crop 16 pixels on all sides to get 256x256 images\n",
    "model = fcn.build_model(crop=16, use_bn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create paths\n",
    "This cell just creates a few paths used later to save training output (e.g. the model architecture, training results and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(\n",
    "    settings.MODEL_PATH, \"FCN_baseline_\" + fcn.title + \"_ep{}.model\".format(settings.TRN_NUM_EPOCH))\n",
    "summ_path = os.path.join(\n",
    "    settings.OUTPUT_PATH, \"FCN_baseline_\" + fcn.title + \"_model_summary.txt\")\n",
    "csv_path = os.path.join(\n",
    "    settings.OUTPUT_PATH, \"FCN_baseline_\" + fcn.title + \"_training_ep{}_bs{}.csv\".format(settings.TRN_NUM_EPOCH,\n",
    "                                                                                    settings.TRN_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/print model architecture information\n",
    "Save the model's architecture to a file, print it in the cell below and save a diagram to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 8, 8, 4096)        16384     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 8, 4096)        16384     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 2)           8194      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 288, 288, 2)       16384     \n",
      "_________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)    (None, 256, 256, 2)       0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256, 256, 2)       0         \n",
      "=================================================================\n",
      "Total params: 134,333,634\n",
      "Trainable params: 134,308,802\n",
      "Non-trainable params: 24,832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model_summary_to_file(model, summ_path)\n",
    "model_architecture_to_file(fcn.model, settings.OUTPUT_PATH + \"FCN_baseline\" + fcn.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "Set the loss function, optimiser and metric and compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the optimiser, loss function and metrics\n",
    "opt = Adam(amsgrad=False)\n",
    "# opt = Adadelta()\n",
    "metrics = [dice_coef]\n",
    "loss = weighted_pixelwise_crossentropy_loss(class_weights)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "The two cells below load the training data. Depending on the value of `USE_GENERATORS` data is either loaded using a generator or loaded into memory. The latter option is useful during development. The former option is almost always required when training with a large(r) number of training images to avoid out of memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Use generators\n",
    "Nothing is actually loaded at this point. Only the generators are prepared for use with `fit_generator()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data generators.\n",
      "\n",
      "  Number of training samples: 9\n",
      "             steps_per_epoch: 9.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare generators, only when NOT using cross validation\n",
    "if USE_GENERATORS:\n",
    "    print(\"Using data generators.\\n\")\n",
    "    \n",
    "    # Training set generator\n",
    "    rdr_train = HDF5Generator_Segment(hdf5_paths[0], hdf5_paths[1],\n",
    "                                      batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                      num_classes=settings.NUM_CLASSES,\n",
    "                                      converter=convert_img_to_pred, \n",
    "                                      feat_key=settings.HDF5_KEY)\n",
    "    gen_train = rdr_train.generator(num_epochs=settings.TRN_NUM_EPOCH)\n",
    "\n",
    "    print(\"  Number of training samples: {}\".format(rdr_train.num_images()))\n",
    "    print(\"             steps_per_epoch: {}\".format(rdr_train.num_images()/\n",
    "                                                    settings.TRN_BATCH_SIZE))\n",
    "\n",
    "\n",
    "    if USE_VALIDATION_SET:\n",
    "        # Validation set generator\n",
    "        rdr_val = HDF5Generator_Segment(hdf5_paths[2], hdf5_paths[3],\n",
    "                                        batch_size=settings.TRN_BATCH_SIZE,\n",
    "                                        num_classes=settings.NUM_CLASSES,\n",
    "                                        converter=convert_img_to_pred, \n",
    "                                        feat_key=settings.HDF5_KEY)\n",
    "        gen_val = rdr_val.generator(num_epochs=settings.TRN_NUM_EPOCH)\n",
    "\n",
    "        print(\"Number of validation samples: {}\".format(rdr_val.num_images()))\n",
    "        print(\"            validation_steps: {}\".format(rdr_val.num_images()/\n",
    "                                                        settings.TRN_BATCH_SIZE))\n",
    "else:\n",
    "    print(\"NOT using generators.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Load all data into memory\n",
    "All data is loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT loading all data into memory.\n"
     ]
    }
   ],
   "source": [
    "if not USE_GENERATORS:\n",
    "    print(\"Loading all data into memory.\\n\")\n",
    "\n",
    "    train_imgs = read_images(hdf5_paths[0], settings.HDF5_KEY)\n",
    "    train_grndtr = read_groundtruths(hdf5_paths[1], settings.HDF5_KEY)\n",
    "    train_grndtr_ext_conv = convert_img_to_pred(train_grndtr,\n",
    "                                                settings.NUM_CLASSES,\n",
    "                                                settings.VERBOSE)\n",
    "    print(\"  Number of training samples: {}\\n\".format(len(train_imgs)))\n",
    "\n",
    "    # Read the validation set (only when NOT using cross-validation)\n",
    "    if not USE_KFOLD_CV:\n",
    "        val_imgs = read_images(hdf5_paths[2], settings.HDF5_KEY)\n",
    "        val_grndtr = read_groundtruths(hdf5_paths[3], settings.HDF5_KEY)\n",
    "        val_grndtr_ext_conv = convert_img_to_pred(val_grndtr,\n",
    "                                                  settings.NUM_CLASSES,\n",
    "                                                  settings.VERBOSE)\n",
    "        print(\"Number of validation samples: {}\\n\".format(len(val_imgs)))\n",
    "    else:\n",
    "        print(\"Using cross validation, not loading a validation set.\")\n",
    "else:\n",
    "    print(\"NOT loading all data into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Execute the training process, either using generators or using data loaded into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Train using generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without a validation set, using generators.\n",
      "\n",
      "Generic information:\n",
      "              Model: FCN-32\n",
      "          Saving to: ../savedmodels/FCN_baseline_FCN-32_ep100.model\n",
      " Number of patients: 1\n",
      "     Training shape: (9, 256, 256, 1) (9 per patient)\n",
      "   Validation shape: None (0 per patient)\n",
      "      Class weights: [1.0, 8.0]\n",
      "\n",
      "Hyper parameters:\n",
      "          Optimizer: <class 'keras.optimizers.Adam'>\n",
      "                   : 0 = ('lr', 0.0010000000474974513)\n",
      "                   : 1 = ('beta_1', 0.8999999761581421)\n",
      "                   : 2 = ('beta_2', 0.9990000128746033)\n",
      "                   : 3 = ('decay', 0.0)\n",
      "                   : 4 = ('epsilon', 1e-07)\n",
      "                   : 5 = ('amsgrad', False)\n",
      "               Loss: <function weighted_pixelwise_crossentropy_loss\n",
      "         IMG_HEIGHT: 256\n",
      "          IMG_WIDTH: 256\n",
      "       IMG_CHANNELS: 1\n",
      "        NUM_CLASSES: 2\n",
      "        SLICE_START: 59\n",
      "          SLICE_END: 69\n",
      "    IMG_CROP_HEIGHT: 32\n",
      "     IMG_CROP_WIDTH: 32\n",
      "\n",
      "     TRN_BATCH_SIZE: 1\n",
      "  TRN_LEARNING_RATE: 0.001\n",
      "      TRN_NUM_EPOCH: 100\n",
      "TRN_TRAIN_VAL_SPLIT: 0.1\n",
      "   TRN_DROPOUT_RATE: 0.5\n",
      "       TRN_MOMENTUM: 0.99\n",
      " TRN_PRED_THRESHOLD: 0.5\n",
      " TRN_EARLY_PATIENCE: 10\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-61484ea00dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                                    callbacks=callbacks)\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nElapsed training time: {:.2f} min.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML3-DL-OPENCV/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if USE_GENERATORS:\n",
    "    start_time = time.time()\n",
    "\n",
    "    if USE_VALIDATION_SET:\n",
    "        # Prepare callbacks\n",
    "        callbacks = [ModelCheckpoint(model_path,\n",
    "                                     monitor=\"val_loss\",\n",
    "                                     mode=\"min\",\n",
    "                                     save_best_only=True,\n",
    "                                     verbose=1),\n",
    "                     EarlyStopping(monitor='val_loss',\n",
    "                                   min_delta=0,\n",
    "                                   patience=settings.TRN_EARLY_PATIENCE,\n",
    "                                   verbose=0,\n",
    "                                   mode=\"auto\"),\n",
    "                     CSVLogger(csv_path,\n",
    "                               append=False)]\n",
    "\n",
    "        print(\"Training with a validation set, using generators.\")\n",
    "        print_training_info(fcn, model_path, rdr_train.img_shape, rdr_val.img_shape,\n",
    "                            settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "        # Fit the model using generators and a validation set\n",
    "        hist = model.fit_generator(gen_train,\n",
    "                                   epochs=settings.TRN_NUM_EPOCH,\n",
    "                                   steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                                   verbose=2,\n",
    "                                   validation_data=gen_val,\n",
    "                                   validation_steps=rdr_val.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                                   shuffle=True,\n",
    "                                   callbacks=callbacks)\n",
    "    else:\n",
    "        callbacks = [ModelCheckpoint(model_path,\n",
    "                                     monitor=\"loss\",\n",
    "                                     mode=\"min\", \n",
    "                                     save_best_only=True,\n",
    "                                     verbose=1),\n",
    "                     EarlyStopping(monitor='loss',\n",
    "                                   min_delta=0,\n",
    "                                   patience=settings.TRN_EARLY_PATIENCE,\n",
    "                                   verbose=0,\n",
    "                                   mode=\"auto\"),\n",
    "                     CSVLogger(csv_path,\n",
    "                               append=False)]\n",
    "        \n",
    "        print(\"Training without a validation set, using generators.\")\n",
    "        print_training_info(fcn, model_path, rdr_train.img_shape,\n",
    "                        None, settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "        # Fit the model using a training set only\n",
    "        start_time = time.time()\n",
    "        hist = model.fit_generator(gen_train,\n",
    "                                   epochs=settings.TRN_NUM_EPOCH,\n",
    "                                   steps_per_epoch=rdr_train.num_images()/settings.TRN_BATCH_SIZE,\n",
    "                                   verbose=2,\n",
    "                                   shuffle=True,\n",
    "                                   callbacks=callbacks)\n",
    "\n",
    "    print(\"\\n\\nElapsed training time: {:.2f} min.\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Train using all data loaded in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_GENERATORS:\n",
    "    start_time = time.time()\n",
    "\n",
    "    if USE_VALIDATION_SET:\n",
    "        # Prepare callbacks\n",
    "        callbacks = [ModelCheckpoint(model_path,\n",
    "                                     monitor=\"val_loss\",\n",
    "                                     mode=\"min\",\n",
    "                                     save_best_only=True,\n",
    "                                     verbose=1),\n",
    "            EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=settings.TRN_EARLY_PATIENCE,\n",
    "                          verbose=0,\n",
    "                          mode=\"auto\"),\n",
    "            CSVLogger(csv_path,\n",
    "                      append=False)]\n",
    "\n",
    "        print(\"Training with a validation set, using all data in memory.\")\n",
    "        print_training_info(fcn, model_path, train_imgs.shape, val_imgs.shape,\n",
    "                            settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "        # Fit the model using a training set only\n",
    "        hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                         epochs=settings.TRN_NUM_EPOCH,\n",
    "                         batch_size=settings.TRN_BATCH_SIZE,\n",
    "                         verbose=2,\n",
    "                         shuffle=True,\n",
    "                         validation_data=(val_imgs, val_grndtr_ext_conv),\n",
    "                         callbacks=callbacks)\n",
    "\n",
    "    else:\n",
    "        # Prepare callbacks\n",
    "        callbacks = [ModelCheckpoint(model_path,\n",
    "                                     monitor=\"loss\",\n",
    "                                     mode=\"min\",\n",
    "                                     save_best_only=True,\n",
    "                                     verbose=1),\n",
    "                     EarlyStopping(monitor='loss',\n",
    "                                   min_delta=0,\n",
    "                                   patience=settings.TRN_EARLY_PATIENCE,\n",
    "                                   verbose=0,\n",
    "                                   mode=\"auto\"),\n",
    "                     CSVLogger(csv_path,\n",
    "                               append=False)]\n",
    "\n",
    "        print(\"Training without a validation set, using all data in memory.\")\n",
    "        print_training_info(fcn, model_path, train_imgs.shape, None,\n",
    "                            settings, class_weights, num_patients, opt, loss)\n",
    "\n",
    "        # Fit the model using a training set only\n",
    "        start_time = time.time()\n",
    "        hist = model.fit(train_imgs, train_grndtr_ext_conv,\n",
    "                         epochs=settings.TRN_NUM_EPOCH,\n",
    "                         batch_size=settings.TRN_BATCH_SIZE,\n",
    "                         verbose=2,\n",
    "                         shuffle=True,\n",
    "                         callbacks=callbacks)\n",
    "\n",
    "    print(\"\\n\\nElapsed training time: {:.2f} min.\".format(int((time.time() - start_time))/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot/save the training results\n",
    "Show a plot of the training loss and Dice coefficient by epoch and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not USE_KFOLD_CV:\n",
    "    plot_training_history(hist,\n",
    "                          show=True,\n",
    "                          save_path=settings.OUTPUT_PATH + fcn.title,\n",
    "                          time_stamp=True,\n",
    "                          metric=\"dice_coef\")\n",
    "else:\n",
    "    print(\"Using cross-validation, no training history saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform pipeline test\n",
    "Use the trained model on one sample in the training data set. This is just to perform pipeline testing during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read training images and ground truths\n",
    "train_imgs = read_images(hdf5_paths[0], settings.HDF5_KEY)\n",
    "train_grndtr = read_groundtruths(hdf5_paths[1], settings.HDF5_KEY)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(train_imgs, batch_size=settings.TRN_BATCH_SIZE, verbose=2)\n",
    "print(\"Predictions shape: {}\".format(predictions.shape))\n",
    "\n",
    "# Convert predictions to images\n",
    "predictions_imgs = convert_pred_to_img(predictions,\n",
    "                                       threshold=settings.TRN_PRED_THRESHOLD,\n",
    "                                       verbose=settings.VERBOSE)\n",
    "\n",
    "# Show a single image, ground truth and segmentation map\n",
    "show_image(np.squeeze(predictions_imgs[0]), 'Segmentation map')\n",
    "show_image(np.squeeze(train_grndtr[0]), 'Ground truth')\n",
    "show_image(np.squeeze(train_imgs[0]), 'Original image')\n",
    "\n",
    "print(\"segmentation map {} dtype {}\".format(np.max(predictions_imgs[0]),\n",
    "                                            predictions_imgs[0].dtype))\n",
    "print(\"    ground truth {} dtype {}\".format(np.max(train_grndtr[0]),\n",
    "                                            train_grndtr[0].dtype))\n",
    "print(\"        original {} dtype {}\".format(np.max(train_imgs[0]),\n",
    "                                            train_imgs[0].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Training complete\n",
    "The trained model is now ready to be applied to test MRI images using `FCN_baseline_test.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Shelhamer]*: Shelhamer, Evan and Long, Jonathan and Darrell, Trevor. Fully convolutional networks for semantic segmentation. In *IEEE transactions on pattern analysis and machine intelligence*, pages 640-651. IEEE, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore code below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.layers import *\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class BilinearUpSampling2D(Layer):\n",
    "    def __init__(self, size=(1, 1), **kwargs):\n",
    "        data_format = K.image_data_format()\n",
    "        self.size = tuple(size)\n",
    "        self.target_size = None\n",
    "\n",
    "        assert data_format in {'channels_last', 'channels_first'}, 'data_format must be in {tf, th}'\n",
    "        self.data_format = data_format\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        width = int(self.size[0] * input_shape[1] if input_shape[1] is not None else None)\n",
    "        height = int(self.size[1] * input_shape[2] if input_shape[2] is not None else None)\n",
    "\n",
    "        return (input_shape[0], width, height, input_shape[3])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def FCN_Vgg16_32s(input_shape=None, weight_decay=0., batch_momentum=0.9, batch_shape=None, classes=21):\n",
    "    if batch_shape:\n",
    "        img_input = Input(batch_shape=batch_shape)\n",
    "        image_size = batch_shape[1:3]\n",
    "    else:\n",
    "        img_input = Input(shape=input_shape)\n",
    "        image_size = input_shape[0:2]\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', kernel_regularizer=l2(weight_decay))(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    # Convolutional layers transfered from fully-connected layers\n",
    "    x = Conv2D(4096, (7, 7), activation='relu', padding='same', name='fc1', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(4096, (1, 1), activation='relu', padding='same', name='fc2', kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #classifying layer\n",
    "    x = Conv2D(classes, (1, 1), kernel_initializer='he_normal', activation='linear', padding='valid', strides=(1, 1), kernel_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "    x = BilinearUpSampling2D(size=(32, 32))(x)\n",
    "\n",
    "    model = Model(img_input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = FCN_Vgg16_32s((256, 256, 1), classes=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def FCN32( n_classes ,  input_height=416, input_width=608 , vgg_level=3):\n",
    "\n",
    "\tassert input_height%32 == 0\n",
    "\tassert input_width%32 == 0\n",
    "\n",
    "\t# https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5\n",
    "\timg_input = Input(shape=(3,input_height,input_width))\n",
    "\n",
    "\tx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format=IMAGE_ORDERING )(img_input)\n",
    "\tx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf1 = x\n",
    "\t# Block 2\n",
    "\tx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf2 = x\n",
    "\n",
    "\t# Block 3\n",
    "\tx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf3 = x\n",
    "\n",
    "\t# Block 4\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf4 = x\n",
    "\n",
    "\t# Block 5\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf5 = x\n",
    "\n",
    "\tx = Flatten(name='flatten')(x)\n",
    "\tx = Dense(4096, activation='relu', name='fc1')(x)\n",
    "\tx = Dense(4096, activation='relu', name='fc2')(x)\n",
    "\tx = Dense( n_classes , activation='softmax', name='predictions')(x)\n",
    "\n",
    "\tvgg  = Model(  img_input , x  )\n",
    "# \tvgg.load_weights(VGG_Weights_path)\n",
    "\n",
    "\to = f5\n",
    "\n",
    "\to = ( Conv2D( 4096 , ( 7 , 7 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(o)\n",
    "\to = Dropout(0.5)(o)\n",
    "\to = ( Conv2D( 4096 , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(o)\n",
    "\to = Dropout(0.5)(o)\n",
    "\n",
    "\to = ( Conv2D( n_classes ,  ( 1 , 1 ) ,kernel_initializer='he_normal' , data_format=IMAGE_ORDERING))(o)\n",
    "\to = Conv2DTranspose( n_classes , kernel_size=(64,64) ,  strides=(32,32) , use_bias=False ,  data_format=IMAGE_ORDERING )(o)\n",
    "\to_shape = Model(img_input , o ).output_shape\n",
    "\t\n",
    "\toutputHeight = o_shape[2]\n",
    "\toutputWidth = o_shape[3]\n",
    "\n",
    "\tprint(\"koko\" , o_shape)\n",
    "\n",
    "\to = (Reshape(( -1  , outputHeight*outputWidth   )))(o)\n",
    "\to = (Permute((2, 1)))(o)\n",
    "\to = (Activation('softmax'))(o)\n",
    "\tmodel = Model( img_input , o )\n",
    "\tmodel.outputWidth = outputWidth\n",
    "\tmodel.outputHeight = outputHeight\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IMAGE_ORDERING = 'channels_last'\n",
    "lala = FCN32(2, 256, 256)\n",
    "lala.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dropout, Permute, Add, add\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Deconvolution2D, Cropping2D\n",
    "\n",
    "\n",
    "def convblock(cdim, nb, bits=3):\n",
    "\tL = []\n",
    "\n",
    "\tfor k in range(1, bits + 1):\n",
    "\t\tconvname = 'conv' + str(nb) + '_' + str(k)\n",
    "\t\tif False:\n",
    "\t\t\t# first version I tried\n",
    "\t\t\tL.append(ZeroPadding2D((1, 1)))\n",
    "\t\t\tL.append(Convolution2D(cdim, kernel_size=(3, 3), activation='relu', name=convname))\n",
    "\t\telse:\n",
    "\t\t\tL.append(Convolution2D(cdim, kernel_size=(3, 3), padding='same', activation='relu', name=convname))\n",
    "\n",
    "\tL.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\treturn L\n",
    "\n",
    "\n",
    "def fcn32_blank(image_size=512):\n",
    "\twithDO = False  # no effect during evaluation but usefull for fine-tuning\n",
    "\n",
    "\tif True:\n",
    "\t\tmdl = Sequential()\n",
    "\n",
    "\t\t# First layer is a dummy-permutation = Identity to specify input shape\n",
    "\t\tmdl.add(Permute((1, 2, 3), input_shape=(image_size, image_size, 3)))  # WARNING : axis 0 is the sample dim\n",
    "\n",
    "\t\tfor l in convblock(64, 1, bits=2):\n",
    "\t\t\tmdl.add(l)\n",
    "\n",
    "\t\tfor l in convblock(128, 2, bits=2):\n",
    "\t\t\tmdl.add(l)\n",
    "\n",
    "\t\tfor l in convblock(256, 3, bits=3):\n",
    "\t\t\tmdl.add(l)\n",
    "\n",
    "\t\tfor l in convblock(512, 4, bits=3):\n",
    "\t\t\tmdl.add(l)\n",
    "\n",
    "\t\tfor l in convblock(512, 5, bits=3):\n",
    "\t\t\tmdl.add(l)\n",
    "\n",
    "\t\tmdl.add(Convolution2D(4096, kernel_size=(7, 7), padding='same', activation='relu', name='fc6'))  # WARNING border\n",
    "\t\tif withDO:\n",
    "\t\t\tmdl.add(Dropout(0.5))\n",
    "\t\tmdl.add(Convolution2D(4096, kernel_size=(1, 1), padding='same', activation='relu', name='fc7'))  # WARNING border\n",
    "\t\tif withDO:\n",
    "\t\t\tmdl.add(Dropout(0.5))\n",
    "\n",
    "\t\t# WARNING : model decapitation i.e. remove the classifier step of VGG16 (usually named fc8)\n",
    "\n",
    "\t\tmdl.add(Convolution2D(21, kernel_size=(1, 1), padding='same', activation='relu', name='score_fr'))\n",
    "\n",
    "\t\tconvsize = mdl.layers[-1].output_shape[2]\n",
    "\t\tprint(\"convsize: \", convsize)\n",
    "\t\tdeconv_output_size = (convsize - 1) * 2 + 4  # INFO: =34 when images are 512x512\n",
    "\t\tprint(\"deconv_output_size: \", deconv_output_size)\n",
    "\t\t# WARNING : valid, same or full ?\n",
    "\t\tmdl.add(Deconvolution2D(21, kernel_size=(4, 4), strides=(2, 2), padding='valid', activation=None, name='score2'))\n",
    "\n",
    "\t\textra_margin = deconv_output_size - convsize * 2  # INFO: =2 when images are 512x512\n",
    "\t\tprint(\"extra_margin\", extra_margin)\n",
    "\t\tassert (extra_margin > 0)\n",
    "\t\tassert (extra_margin % 2 == 0)\n",
    "\t\t# INFO : cropping as deconv gained pixels\n",
    "\t\t# print(extra_margin)\n",
    "\t\tc = ((0, extra_margin), (0, extra_margin))\n",
    "\t\t# print(c)\n",
    "\t\tmdl.add(Cropping2D(cropping=c))\n",
    "\t\t# print(mdl.summary())\n",
    "\n",
    "\t\treturn mdl\n",
    "\n",
    "\telse:\n",
    "\t\t# See following link for a version based on Keras functional API :\n",
    "\t\t# gist.github.com/EncodeTS/6bbe8cb8bebad7a672f0d872561782d9\n",
    "\t\traise ValueError('not implemented')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lala = fcn32_blank(256)\n",
    "lala.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
